
==> Í∞êÏÇ¨ <==
|-----------|-----------------------|----------|---------|---------|---------------------|---------------------|
|  Command  |         Args          | Profile  |  User   | Version |     Start Time      |      End Time       |
|-----------|-----------------------|----------|---------|---------|---------------------|---------------------|
| start     |                       | minikube | cho-cho | v1.35.0 | 02 Apr 25 20:29 KST | 02 Apr 25 20:30 KST |
| addons    | enable ingress        | minikube | cho-cho | v1.35.0 | 02 Apr 25 20:30 KST | 02 Apr 25 20:31 KST |
| dashboard |                       | minikube | cho-cho | v1.35.0 | 02 Apr 25 20:43 KST |                     |
| addons    | enable metrics-server | minikube | cho-cho | v1.35.0 | 02 Apr 25 20:43 KST | 02 Apr 25 20:43 KST |
| dashboard |                       | minikube | cho-cho | v1.35.0 | 02 Apr 25 20:43 KST |                     |
| dashboard | --url                 | minikube | cho-cho | v1.35.0 | 02 Apr 25 20:46 KST |                     |
| addons    | list                  | minikube | cho-cho | v1.35.0 | 02 Apr 25 20:47 KST | 02 Apr 25 20:47 KST |
| addons    | enable dashboard      | minikube | cho-cho | v1.35.0 | 02 Apr 25 20:47 KST | 02 Apr 25 20:47 KST |
| addons    | enable dashboard      | minikube | cho-cho | v1.35.0 | 02 Apr 25 20:47 KST | 02 Apr 25 20:47 KST |
| dashboard | --url                 | minikube | cho-cho | v1.35.0 | 02 Apr 25 20:47 KST |                     |
| dashboard |                       | minikube | cho-cho | v1.35.0 | 02 Apr 25 20:49 KST |                     |
| dashboard |                       | minikube | cho-cho | v1.35.0 | 03 Apr 25 16:19 KST |                     |
|-----------|-----------------------|----------|---------|---------|---------------------|---------------------|


==> ÎßàÏßÄÎßâ ÏãúÏûë <==
Log file created at: 2025/04/02 20:29:22
Running on machine: cho-choui-MacBookAir
Binary: Built with gc go1.23.4 for darwin/arm64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I0402 20:29:22.111804   56533 out.go:345] Setting OutFile to fd 1 ...
I0402 20:29:22.111947   56533 out.go:397] isatty.IsTerminal(1) = true
I0402 20:29:22.111949   56533 out.go:358] Setting ErrFile to fd 2...
I0402 20:29:22.111951   56533 out.go:397] isatty.IsTerminal(2) = true
I0402 20:29:22.112071   56533 root.go:338] Updating PATH: /Users/cho-cho/.minikube/bin
W0402 20:29:22.112139   56533 root.go:314] Error reading config file at /Users/cho-cho/.minikube/config/config.json: open /Users/cho-cho/.minikube/config/config.json: no such file or directory
I0402 20:29:22.113153   56533 out.go:352] Setting JSON to false
I0402 20:29:22.142476   56533 start.go:129] hostinfo: {"hostname":"cho-choui-MacBookAir.local","uptime":2594886,"bootTime":1740998476,"procs":516,"os":"darwin","platform":"darwin","platformFamily":"Standalone Workstation","platformVersion":"14.4.1","kernelVersion":"23.4.0","kernelArch":"arm64","virtualizationSystem":"","virtualizationRole":"","hostId":"f87944dd-610a-5cd7-b856-78372786e2ae"}
W0402 20:29:22.142600   56533 start.go:137] gopshost.Virtualization returned error: not implemented yet
I0402 20:29:22.149611   56533 out.go:177] üòÑ  Darwin 14.4.1 (arm64) Ïùò minikube v1.35.0
I0402 20:29:22.158213   56533 notify.go:220] Checking for updates...
I0402 20:29:22.159286   56533 driver.go:394] Setting default libvirt URI to qemu:///system
W0402 20:29:22.159275   56533 preload.go:293] Failed to list preload files: open /Users/cho-cho/.minikube/cache/preloaded-tarball: no such file or directory
I0402 20:29:22.159512   56533 global.go:112] Querying for installed drivers using PATH=/Users/cho-cho/.minikube/bin:/usr/local/mysql/bin:/usr/local/opt/tcl-tk/bin:/Users/cho-cho/.asdf/shims:/opt/homebrew/opt/asdf/libexec/bin:/opt/homebrew/Caskroom/miniforge/base/condabin:/opt/local/bin:/opt/local/sbin:/opt/homebrew/bin:/opt/homebrew/sbin:/usr/local/bin:/System/Cryptexes/App/usr/bin:/usr/bin:/bin:/usr/sbin:/sbin:/var/run/com.apple.security.cryptexd/codex.system/bootstrap/usr/local/bin:/var/run/com.apple.security.cryptexd/codex.system/bootstrap/usr/bin:/var/run/com.apple.security.cryptexd/codex.system/bootstrap/usr/appleinternal/bin:/Library/Apple/usr/bin:/Library/TeX/texbin:/usr/local/share/dotnet:~/.dotnet/tools:/Users/cho-cho/.cargo/bin:/Applications/iTerm.app/Contents/Resources/utilities:/opt/homebrew/opt/fzf/bin
I0402 20:29:22.160646   56533 global.go:133] hyperkit default: true priority: 8, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:exec: "hyperkit": executable file not found in $PATH Reason: Fix:Run 'brew install hyperkit' Doc:https://minikube.sigs.k8s.io/docs/reference/drivers/hyperkit/ Version:}
I0402 20:29:22.161174   56533 global.go:133] qemu2 default: true priority: 7, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:exec: "qemu-system-aarch64": executable file not found in $PATH Reason: Fix:Install qemu-system Doc:https://minikube.sigs.k8s.io/docs/reference/drivers/qemu/ Version:}
I0402 20:29:22.161253   56533 global.go:133] podman default: true priority: 3, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:exec: "podman": executable file not found in $PATH Reason: Fix:Install Podman Doc:https://minikube.sigs.k8s.io/docs/drivers/podman/ Version:}
I0402 20:29:22.161272   56533 global.go:133] ssh default: false priority: 4, state: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0402 20:29:22.161507   56533 global.go:133] parallels default: true priority: 7, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:exec: "prlctl": executable file not found in $PATH Reason: Fix:Install Parallels Desktop for Mac Doc:https://minikube.sigs.k8s.io/docs/drivers/parallels/ Version:}
I0402 20:29:22.161624   56533 global.go:133] vfkit default: true priority: 3, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:exec: "vfkit": executable file not found in $PATH Reason: Fix:Run 'brew tap cfergeau/crc && brew install vfkit' Doc:https://minikube.sigs.k8s.io/docs/reference/drivers/vfkit/ Version:}
I0402 20:29:22.161767   56533 global.go:133] virtualbox default: true priority: 6, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:unable to find VBoxManage in $PATH Reason: Fix:Install VirtualBox Doc:https://minikube.sigs.k8s.io/docs/reference/drivers/virtualbox/ Version:}
I0402 20:29:22.161833   56533 global.go:133] vmware default: false priority: 5, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:exec: "vmrun": executable file not found in $PATH Reason: Fix:Install vmrun Doc:https://minikube.sigs.k8s.io/docs/reference/drivers/vmware/ Version:}
I0402 20:29:22.294668   56533 docker.go:123] docker version: linux-20.10.13:Docker Desktop 4.6.1 (76265)
I0402 20:29:22.295187   56533 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0402 20:29:22.694648   56533 info.go:266] docker info: {ID:UIDC:OSEP:BFQB:7W3K:M7D6:EVFQ:BMPC:GXGC:YIY2:UKMP:ASNP:7BSC Containers:0 ContainersRunning:0 ContainersPaused:0 ContainersStopped:0 Images:12 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:46 OomKillDisable:false NGoroutines:48 SystemTime:2025-04-02 11:29:22.388647716 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:4 KernelVersion:5.10.104-linuxkit OperatingSystem:Docker Desktop OSType:linux Architecture:aarch64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:4 MemTotal:8337530880 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[] ExperimentalBuild:false ServerVersion:20.10.13 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:2a1d4dbdb2a1030dc5b01e96fb110a9d9f150ecc Expected:2a1d4dbdb2a1030dc5b01e96fb110a9d9f150ecc} RuncCommit:{ID:v1.0.3-0-gf46b6ba Expected:v1.0.3-0-gf46b6ba} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=default name=cgroupns] ProductLicense: Warnings:<nil> ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/usr/local/lib/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.8.1] map[Name:compose Path:/usr/local/lib/docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.3.3] map[Name:scan Path:/usr/local/lib/docker/cli-plugins/docker-scan SchemaVersion:0.1.0 ShortDescription:Docker Scan Vendor:Docker Inc. Version:v0.17.0]] Warnings:<nil>}}
I0402 20:29:22.694761   56533 global.go:133] docker default: true priority: 9, state: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0402 20:29:22.694948   56533 driver.go:316] not recommending "ssh" due to default: false
I0402 20:29:22.694978   56533 driver.go:351] Picked: docker
I0402 20:29:22.694982   56533 driver.go:352] Alternatives: [ssh]
I0402 20:29:22.694989   56533 driver.go:353] Rejects: [hyperkit qemu2 podman parallels vfkit virtualbox vmware]
I0402 20:29:22.703607   56533 out.go:177] ‚ú®  ÏûêÎèôÏ†ÅÏúºÎ°ú docker ÎìúÎùºÏù¥Î≤ÑÍ∞Ä ÏÑ†ÌÉùÎêòÏóàÏäµÎãàÎã§
I0402 20:29:22.706819   56533 start.go:297] selected driver: docker
I0402 20:29:22.706995   56533 start.go:901] validating driver "docker" against <nil>
I0402 20:29:22.707003   56533 start.go:912] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0402 20:29:22.707179   56533 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0402 20:29:22.873103   56533 info.go:266] docker info: {ID:UIDC:OSEP:BFQB:7W3K:M7D6:EVFQ:BMPC:GXGC:YIY2:UKMP:ASNP:7BSC Containers:0 ContainersRunning:0 ContainersPaused:0 ContainersStopped:0 Images:12 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:46 OomKillDisable:false NGoroutines:48 SystemTime:2025-04-02 11:29:22.80286755 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:4 KernelVersion:5.10.104-linuxkit OperatingSystem:Docker Desktop OSType:linux Architecture:aarch64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:4 MemTotal:8337530880 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[] ExperimentalBuild:false ServerVersion:20.10.13 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:2a1d4dbdb2a1030dc5b01e96fb110a9d9f150ecc Expected:2a1d4dbdb2a1030dc5b01e96fb110a9d9f150ecc} RuncCommit:{ID:v1.0.3-0-gf46b6ba Expected:v1.0.3-0-gf46b6ba} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=default name=cgroupns] ProductLicense: Warnings:<nil> ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/usr/local/lib/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.8.1] map[Name:compose Path:/usr/local/lib/docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.3.3] map[Name:scan Path:/usr/local/lib/docker/cli-plugins/docker-scan SchemaVersion:0.1.0 ShortDescription:Docker Scan Vendor:Docker Inc. Version:v0.17.0]] Warnings:<nil>}}
I0402 20:29:22.873520   56533 start_flags.go:310] no existing cluster config was found, will generate one from the flags 
I0402 20:29:22.873929   56533 start_flags.go:393] Using suggested 4000MB memory alloc based on sys=16384MB, container=7951MB
I0402 20:29:22.874023   56533 start_flags.go:929] Wait components to verify : map[apiserver:true system_pods:true]
I0402 20:29:22.878376   56533 out.go:177] üìå  Using Docker Desktop driver with root privileges
I0402 20:29:22.882358   56533 cni.go:84] Creating CNI manager for ""
I0402 20:29:22.882559   56533 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0402 20:29:22.882562   56533 start_flags.go:319] Found "bridge CNI" CNI - setting NetworkPlugin=cni
I0402 20:29:22.882771   56533 start.go:340] cluster config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 Memory:4000 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.32.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP: Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/Users:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0402 20:29:22.891440   56533 out.go:177] üëç  Starting "minikube" primary control-plane node in "minikube" cluster
I0402 20:29:22.894795   56533 cache.go:121] Beginning downloading kic base image for docker with docker
I0402 20:29:22.898386   56533 out.go:177] üöú  Pulling base image v0.0.46 ...
I0402 20:29:22.906649   56533 preload.go:131] Checking if preload exists for k8s version v1.32.0 and runtime docker
I0402 20:29:22.906794   56533 image.go:81] Checking for gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 in local docker daemon
I0402 20:29:23.031098   56533 image.go:100] Found gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 in local docker daemon, skipping pull
I0402 20:29:23.031267   56533 cache.go:145] gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 exists in daemon, skipping load
I0402 20:29:23.061088   56533 preload.go:118] Found remote preload: https://storage.googleapis.com/minikube-preloaded-volume-tarballs/v18/v1.32.0/preloaded-images-k8s-v18-v1.32.0-docker-overlay2-arm64.tar.lz4
I0402 20:29:23.061104   56533 cache.go:56] Caching tarball of preloaded images
I0402 20:29:23.061236   56533 preload.go:131] Checking if preload exists for k8s version v1.32.0 and runtime docker
I0402 20:29:23.065847   56533 out.go:177] üíæ  Ïø†Î≤ÑÎÑ§Ìã∞Ïä§ v1.32.0 ÏùÑ Îã§Ïö¥Î°úÎìú Ï§ë ...
I0402 20:29:23.072674   56533 preload.go:236] getting checksum for preloaded-images-k8s-v18-v1.32.0-docker-overlay2-arm64.tar.lz4 ...
I0402 20:29:23.341412   56533 download.go:108] Downloading: https://storage.googleapis.com/minikube-preloaded-volume-tarballs/v18/v1.32.0/preloaded-images-k8s-v18-v1.32.0-docker-overlay2-arm64.tar.lz4?checksum=md5:ff0c92f745fa493248e668330d02c326 -> /Users/cho-cho/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.32.0-docker-overlay2-arm64.tar.lz4
I0402 20:29:35.730409   56533 preload.go:247] saving checksum for preloaded-images-k8s-v18-v1.32.0-docker-overlay2-arm64.tar.lz4 ...
I0402 20:29:35.730592   56533 preload.go:254] verifying checksum of /Users/cho-cho/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.32.0-docker-overlay2-arm64.tar.lz4 ...
I0402 20:29:36.387584   56533 cache.go:59] Finished verifying existence of preloaded tar for v1.32.0 on docker
I0402 20:29:36.388950   56533 profile.go:143] Saving config to /Users/cho-cho/.minikube/profiles/minikube/config.json ...
I0402 20:29:36.388989   56533 lock.go:35] WriteFile acquiring /Users/cho-cho/.minikube/profiles/minikube/config.json: {Name:mkeb5d88ee4c99163295832d812b392e176ec148 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0402 20:29:36.389284   56533 cache.go:227] Successfully downloaded all kic artifacts
I0402 20:29:36.390061   56533 start.go:360] acquireMachinesLock for minikube: {Name:mkf750fd23d76c7800c3f5809601b7eb5e9e349c Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0402 20:29:36.390104   56533 start.go:364] duration metric: took 34.25¬µs to acquireMachinesLock for "minikube"
I0402 20:29:36.390121   56533 start.go:93] Provisioning new machine with config: &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 Memory:4000 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.32.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP: Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/Users:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} &{Name: IP: Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}
I0402 20:29:36.390167   56533 start.go:125] createHost starting for "" (driver="docker")
I0402 20:29:36.400039   56533 out.go:235] üî•  Creating docker container (CPUs=2, Memory=4000MB) ...
I0402 20:29:36.400413   56533 start.go:159] libmachine.API.Create for "minikube" (driver="docker")
I0402 20:29:36.400724   56533 client.go:168] LocalClient.Create starting
I0402 20:29:36.401378   56533 main.go:141] libmachine: Creating CA: /Users/cho-cho/.minikube/certs/ca.pem
I0402 20:29:36.459362   56533 main.go:141] libmachine: Creating client certificate: /Users/cho-cho/.minikube/certs/cert.pem
I0402 20:29:36.527435   56533 cli_runner.go:164] Run: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
W0402 20:29:36.662932   56533 cli_runner.go:211] docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}" returned with exit code 1
I0402 20:29:36.663214   56533 network_create.go:284] running [docker network inspect minikube] to gather additional debugging logs...
I0402 20:29:36.663238   56533 cli_runner.go:164] Run: docker network inspect minikube
W0402 20:29:36.823437   56533 cli_runner.go:211] docker network inspect minikube returned with exit code 1
I0402 20:29:36.823461   56533 network_create.go:287] error running [docker network inspect minikube]: docker network inspect minikube: exit status 1
stdout:
[]

stderr:
Error: No such network: minikube
I0402 20:29:36.823473   56533 network_create.go:289] output of [docker network inspect minikube]: -- stdout --
[]

-- /stdout --
** stderr ** 
Error: No such network: minikube

** /stderr **
I0402 20:29:36.823668   56533 cli_runner.go:164] Run: docker network inspect bridge --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I0402 20:29:36.939720   56533 network.go:206] using free private subnet 192.168.49.0/24: &{IP:192.168.49.0 Netmask:255.255.255.0 Prefix:24 CIDR:192.168.49.0/24 Gateway:192.168.49.1 ClientMin:192.168.49.2 ClientMax:192.168.49.254 Broadcast:192.168.49.255 IsPrivate:true Interface:{IfaceName: IfaceIPv4: IfaceMTU:0 IfaceMAC:} reservation:0x14001ab6e70}
I0402 20:29:36.939748   56533 network_create.go:124] attempt to create docker network minikube 192.168.49.0/24 with gateway 192.168.49.1 and MTU of 1500 ...
I0402 20:29:36.939840   56533 cli_runner.go:164] Run: docker network create --driver=bridge --subnet=192.168.49.0/24 --gateway=192.168.49.1 -o --ip-masq -o --icc -o com.docker.network.driver.mtu=1500 --label=created_by.minikube.sigs.k8s.io=true --label=name.minikube.sigs.k8s.io=minikube minikube
I0402 20:29:37.099355   56533 network_create.go:108] docker network minikube 192.168.49.0/24 created
I0402 20:29:37.099398   56533 kic.go:121] calculated static IP "192.168.49.2" for the "minikube" container
I0402 20:29:37.099709   56533 cli_runner.go:164] Run: docker ps -a --format {{.Names}}
I0402 20:29:37.243851   56533 cli_runner.go:164] Run: docker volume create minikube --label name.minikube.sigs.k8s.io=minikube --label created_by.minikube.sigs.k8s.io=true
I0402 20:29:37.360756   56533 oci.go:103] Successfully created a docker volume minikube
I0402 20:29:37.360902   56533 cli_runner.go:164] Run: docker run --rm --name minikube-preload-sidecar --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --entrypoint /usr/bin/test -v minikube:/var gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 -d /var/lib
I0402 20:29:38.290307   56533 oci.go:107] Successfully prepared a docker volume minikube
I0402 20:29:38.290353   56533 preload.go:131] Checking if preload exists for k8s version v1.32.0 and runtime docker
I0402 20:29:38.290377   56533 kic.go:194] Starting extracting preloaded images to volume ...
I0402 20:29:38.290611   56533 cli_runner.go:164] Run: docker run --rm --entrypoint /usr/bin/tar -v /Users/cho-cho/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.32.0-docker-overlay2-arm64.tar.lz4:/preloaded.tar:ro -v minikube:/extractDir gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 -I lz4 -xf /preloaded.tar -C /extractDir
I0402 20:29:52.098777   56533 cli_runner.go:217] Completed: docker run --rm --entrypoint /usr/bin/tar -v /Users/cho-cho/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.32.0-docker-overlay2-arm64.tar.lz4:/preloaded.tar:ro -v minikube:/extractDir gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 -I lz4 -xf /preloaded.tar -C /extractDir: (13.808599208s)
I0402 20:29:52.098885   56533 kic.go:203] duration metric: took 13.809163292s to extract preloaded images to volume ...
I0402 20:29:52.099284   56533 cli_runner.go:164] Run: docker info --format "'{{json .SecurityOptions}}'"
I0402 20:29:52.510344   56533 cli_runner.go:164] Run: docker run -d -t --privileged --security-opt seccomp=unconfined --tmpfs /tmp --tmpfs /run -v /lib/modules:/lib/modules:ro --hostname minikube --name minikube --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --label role.minikube.sigs.k8s.io= --label mode.minikube.sigs.k8s.io=minikube --network minikube --ip 192.168.49.2 --volume minikube:/var --security-opt apparmor=unconfined --memory=4000mb --memory-swap=4000mb --cpus=2 -e container=docker --expose 8443 --publish=127.0.0.1::8443 --publish=127.0.0.1::22 --publish=127.0.0.1::2376 --publish=127.0.0.1::5000 --publish=127.0.0.1::32443 gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279
I0402 20:29:52.947759   56533 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Running}}
I0402 20:29:53.082718   56533 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0402 20:29:53.215313   56533 cli_runner.go:164] Run: docker exec minikube stat /var/lib/dpkg/alternatives/iptables
I0402 20:29:53.409990   56533 oci.go:144] the created container "minikube" has a running status.
I0402 20:29:53.410034   56533 kic.go:225] Creating ssh key for kic: /Users/cho-cho/.minikube/machines/minikube/id_rsa...
I0402 20:29:53.556628   56533 kic_runner.go:191] docker (temp): /Users/cho-cho/.minikube/machines/minikube/id_rsa.pub --> /home/docker/.ssh/authorized_keys (381 bytes)
I0402 20:29:53.721841   56533 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0402 20:29:53.840445   56533 kic_runner.go:93] Run: chown docker:docker /home/docker/.ssh/authorized_keys
I0402 20:29:53.840457   56533 kic_runner.go:114] Args: [docker exec --privileged minikube chown docker:docker /home/docker/.ssh/authorized_keys]
I0402 20:29:54.017668   56533 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0402 20:29:54.123006   56533 machine.go:93] provisionDockerMachine start ...
I0402 20:29:54.123194   56533 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0402 20:29:54.233625   56533 main.go:141] libmachine: Using SSH client type: native
I0402 20:29:54.233812   56533 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x1009f0790] 0x1009f2fd0 <nil>  [] 0s} 127.0.0.1 62319 <nil> <nil>}
I0402 20:29:54.233815   56533 main.go:141] libmachine: About to run SSH command:
hostname
I0402 20:29:54.352615   56533 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0402 20:29:54.352804   56533 ubuntu.go:169] provisioning hostname "minikube"
I0402 20:29:54.352919   56533 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0402 20:29:54.460340   56533 main.go:141] libmachine: Using SSH client type: native
I0402 20:29:54.460487   56533 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x1009f0790] 0x1009f2fd0 <nil>  [] 0s} 127.0.0.1 62319 <nil> <nil>}
I0402 20:29:54.460498   56533 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I0402 20:29:54.576087   56533 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0402 20:29:54.576209   56533 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0402 20:29:54.682408   56533 main.go:141] libmachine: Using SSH client type: native
I0402 20:29:54.682552   56533 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x1009f0790] 0x1009f2fd0 <nil>  [] 0s} 127.0.0.1 62319 <nil> <nil>}
I0402 20:29:54.682558   56533 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I0402 20:29:54.792479   56533 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0402 20:29:54.792498   56533 ubuntu.go:175] set auth options {CertDir:/Users/cho-cho/.minikube CaCertPath:/Users/cho-cho/.minikube/certs/ca.pem CaPrivateKeyPath:/Users/cho-cho/.minikube/certs/ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:/Users/cho-cho/.minikube/machines/server.pem ServerKeyPath:/Users/cho-cho/.minikube/machines/server-key.pem ClientKeyPath:/Users/cho-cho/.minikube/certs/key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:/Users/cho-cho/.minikube/certs/cert.pem ServerCertSANs:[] StorePath:/Users/cho-cho/.minikube}
I0402 20:29:54.792519   56533 ubuntu.go:177] setting up certificates
I0402 20:29:54.792529   56533 provision.go:84] configureAuth start
I0402 20:29:54.792647   56533 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0402 20:29:54.900453   56533 provision.go:143] copyHostCerts
I0402 20:29:54.900775   56533 exec_runner.go:151] cp: /Users/cho-cho/.minikube/certs/key.pem --> /Users/cho-cho/.minikube/key.pem (1675 bytes)
I0402 20:29:54.901100   56533 exec_runner.go:151] cp: /Users/cho-cho/.minikube/certs/ca.pem --> /Users/cho-cho/.minikube/ca.pem (1082 bytes)
I0402 20:29:54.901327   56533 exec_runner.go:151] cp: /Users/cho-cho/.minikube/certs/cert.pem --> /Users/cho-cho/.minikube/cert.pem (1123 bytes)
I0402 20:29:54.901541   56533 provision.go:117] generating server cert: /Users/cho-cho/.minikube/machines/server.pem ca-key=/Users/cho-cho/.minikube/certs/ca.pem private-key=/Users/cho-cho/.minikube/certs/ca-key.pem org=cho-cho.minikube san=[127.0.0.1 192.168.49.2 localhost minikube]
I0402 20:29:54.994629   56533 provision.go:177] copyRemoteCerts
I0402 20:29:54.995292   56533 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I0402 20:29:54.995380   56533 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0402 20:29:55.100671   56533 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:62319 SSHKeyPath:/Users/cho-cho/.minikube/machines/minikube/id_rsa Username:docker}
I0402 20:29:55.181101   56533 ssh_runner.go:362] scp /Users/cho-cho/.minikube/certs/ca.pem --> /etc/docker/ca.pem (1082 bytes)
I0402 20:29:55.200531   56533 ssh_runner.go:362] scp /Users/cho-cho/.minikube/machines/server.pem --> /etc/docker/server.pem (1184 bytes)
I0402 20:29:55.216251   56533 ssh_runner.go:362] scp /Users/cho-cho/.minikube/machines/server-key.pem --> /etc/docker/server-key.pem (1675 bytes)
I0402 20:29:55.230449   56533 provision.go:87] duration metric: took 437.92875ms to configureAuth
I0402 20:29:55.230462   56533 ubuntu.go:193] setting minikube options for container-runtime
I0402 20:29:55.230830   56533 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.32.0
I0402 20:29:55.230925   56533 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0402 20:29:55.377930   56533 main.go:141] libmachine: Using SSH client type: native
I0402 20:29:55.378104   56533 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x1009f0790] 0x1009f2fd0 <nil>  [] 0s} 127.0.0.1 62319 <nil> <nil>}
I0402 20:29:55.378113   56533 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I0402 20:29:55.497201   56533 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I0402 20:29:55.497214   56533 ubuntu.go:71] root file system type: overlay
I0402 20:29:55.497322   56533 provision.go:314] Updating docker unit: /lib/systemd/system/docker.service ...
I0402 20:29:55.497465   56533 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0402 20:29:55.618327   56533 main.go:141] libmachine: Using SSH client type: native
I0402 20:29:55.618517   56533 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x1009f0790] 0x1009f2fd0 <nil>  [] 0s} 127.0.0.1 62319 <nil> <nil>}
I0402 20:29:55.618556   56533 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %s "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I0402 20:29:55.738142   56533 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I0402 20:29:55.738270   56533 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0402 20:29:55.897677   56533 main.go:141] libmachine: Using SSH client type: native
I0402 20:29:55.897829   56533 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x1009f0790] 0x1009f2fd0 <nil>  [] 0s} 127.0.0.1 62319 <nil> <nil>}
I0402 20:29:55.897835   56533 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I0402 20:29:56.470586   56533 main.go:141] libmachine: SSH cmd err, output: <nil>: --- /lib/systemd/system/docker.service	2024-12-17 15:44:16.000000000 +0000
+++ /lib/systemd/system/docker.service.new	2025-04-02 11:29:55.734312010 +0000
@@ -1,46 +1,49 @@
 [Unit]
 Description=Docker Application Container Engine
 Documentation=https://docs.docker.com
-After=network-online.target docker.socket firewalld.service containerd.service time-set.target
-Wants=network-online.target containerd.service
+BindsTo=containerd.service
+After=network-online.target firewalld.service containerd.service
+Wants=network-online.target
 Requires=docker.socket
+StartLimitBurst=3
+StartLimitIntervalSec=60
 
 [Service]
 Type=notify
-# the default is not to use systemd for cgroups because the delegate issues still
-# exists and systemd currently does not support the cgroup feature set required
-# for containers run by docker
-ExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock
-ExecReload=/bin/kill -s HUP $MAINPID
-TimeoutStartSec=0
-RestartSec=2
-Restart=always
+Restart=on-failure
 
-# Note that StartLimit* options were moved from "Service" to "Unit" in systemd 229.
-# Both the old, and new location are accepted by systemd 229 and up, so using the old location
-# to make them work for either version of systemd.
-StartLimitBurst=3
 
-# Note that StartLimitInterval was renamed to StartLimitIntervalSec in systemd 230.
-# Both the old, and new name are accepted by systemd 230 and up, so using the old name to make
-# this option work for either version of systemd.
-StartLimitInterval=60s
+
+# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
+# The base configuration already specifies an 'ExecStart=...' command. The first directive
+# here is to clear out that command inherited from the base configuration. Without this,
+# the command from the base configuration and the command specified here are treated as
+# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
+# will catch this invalid input and refuse to start the service with an error like:
+#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.
+
+# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
+# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
+ExecStart=
+ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
+ExecReload=/bin/kill -s HUP $MAINPID
 
 # Having non-zero Limit*s causes performance problems due to accounting overhead
 # in the kernel. We recommend using cgroups to do container-local accounting.
+LimitNOFILE=infinity
 LimitNPROC=infinity
 LimitCORE=infinity
 
-# Comment TasksMax if your systemd version does not support it.
-# Only systemd 226 and above support this option.
+# Uncomment TasksMax if your systemd version supports it.
+# Only systemd 226 and above support this version.
 TasksMax=infinity
+TimeoutStartSec=0
 
 # set delegate yes so that systemd does not reset the cgroups of docker containers
 Delegate=yes
 
 # kill only the docker process, not all processes in the cgroup
 KillMode=process
-OOMScoreAdjust=-500
 
 [Install]
 WantedBy=multi-user.target
Synchronizing state of docker.service with SysV service script with /lib/systemd/systemd-sysv-install.
Executing: /lib/systemd/systemd-sysv-install enable docker

I0402 20:29:56.470616   56533 machine.go:96] duration metric: took 2.347696666s to provisionDockerMachine
I0402 20:29:56.470627   56533 client.go:171] duration metric: took 20.070857291s to LocalClient.Create
I0402 20:29:56.470657   56533 start.go:167] duration metric: took 20.071201708s to libmachine.API.Create "minikube"
I0402 20:29:56.470664   56533 start.go:293] postStartSetup for "minikube" (driver="docker")
I0402 20:29:56.470675   56533 start.go:322] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I0402 20:29:56.470907   56533 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I0402 20:29:56.471015   56533 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0402 20:29:56.613064   56533 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:62319 SSHKeyPath:/Users/cho-cho/.minikube/machines/minikube/id_rsa Username:docker}
I0402 20:29:56.693555   56533 ssh_runner.go:195] Run: cat /etc/os-release
I0402 20:29:56.696520   56533 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I0402 20:29:56.696536   56533 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I0402 20:29:56.696540   56533 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I0402 20:29:56.696543   56533 info.go:137] Remote host: Ubuntu 22.04.5 LTS
I0402 20:29:56.696737   56533 filesync.go:126] Scanning /Users/cho-cho/.minikube/addons for local assets ...
I0402 20:29:56.696823   56533 filesync.go:126] Scanning /Users/cho-cho/.minikube/files for local assets ...
I0402 20:29:56.696851   56533 start.go:296] duration metric: took 226.194792ms for postStartSetup
I0402 20:29:56.697240   56533 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0402 20:29:56.805279   56533 profile.go:143] Saving config to /Users/cho-cho/.minikube/profiles/minikube/config.json ...
I0402 20:29:56.805863   56533 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I0402 20:29:56.805934   56533 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0402 20:29:56.911124   56533 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:62319 SSHKeyPath:/Users/cho-cho/.minikube/machines/minikube/id_rsa Username:docker}
I0402 20:29:56.991256   56533 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I0402 20:29:56.996310   56533 start.go:128] duration metric: took 20.60711825s to createHost
I0402 20:29:56.996323   56533 start.go:83] releasing machines lock for "minikube", held for 20.607199375s
I0402 20:29:56.996441   56533 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0402 20:29:57.111417   56533 ssh_runner.go:195] Run: cat /version.json
I0402 20:29:57.111479   56533 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0402 20:29:57.111589   56533 ssh_runner.go:195] Run: curl -sS -m 2 https://registry.k8s.io/
I0402 20:29:57.112324   56533 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0402 20:29:57.250877   56533 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:62319 SSHKeyPath:/Users/cho-cho/.minikube/machines/minikube/id_rsa Username:docker}
I0402 20:29:57.251132   56533 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:62319 SSHKeyPath:/Users/cho-cho/.minikube/machines/minikube/id_rsa Username:docker}
I0402 20:29:57.331094   56533 ssh_runner.go:195] Run: systemctl --version
I0402 20:29:57.598987   56533 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I0402 20:29:57.608254   56533 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
I0402 20:29:57.635963   56533 cni.go:230] loopback cni configuration patched: "/etc/cni/net.d/*loopback.conf*" found
I0402 20:29:57.636180   56533 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%p, " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I0402 20:29:57.656067   56533 cni.go:262] disabled [/etc/cni/net.d/100-crio-bridge.conf, /etc/cni/net.d/87-podman-bridge.conflist] bridge cni config(s)
I0402 20:29:57.656094   56533 start.go:495] detecting cgroup driver to use...
I0402 20:29:57.656124   56533 detect.go:187] detected "cgroupfs" cgroup driver on host os
I0402 20:29:57.656761   56533 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I0402 20:29:57.666653   56533 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.10"|' /etc/containerd/config.toml"
I0402 20:29:57.673198   56533 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I0402 20:29:57.679889   56533 containerd.go:146] configuring containerd to use "cgroupfs" as cgroup driver...
I0402 20:29:57.680108   56533 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = false|g' /etc/containerd/config.toml"
I0402 20:29:57.687232   56533 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0402 20:29:57.695836   56533 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I0402 20:29:57.703461   56533 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0402 20:29:57.710482   56533 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I0402 20:29:57.716683   56533 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I0402 20:29:57.723350   56533 ssh_runner.go:195] Run: sh -c "sudo sed -i '/^ *enable_unprivileged_ports = .*/d' /etc/containerd/config.toml"
I0402 20:29:57.729863   56533 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)\[plugins."io.containerd.grpc.v1.cri"\]|&\n\1  enable_unprivileged_ports = true|' /etc/containerd/config.toml"
I0402 20:29:57.737096   56533 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I0402 20:29:57.743646   56533 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I0402 20:29:57.749388   56533 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0402 20:29:57.807059   56533 ssh_runner.go:195] Run: sudo systemctl restart containerd
I0402 20:29:57.881595   56533 start.go:495] detecting cgroup driver to use...
I0402 20:29:57.881618   56533 detect.go:187] detected "cgroupfs" cgroup driver on host os
I0402 20:29:57.881825   56533 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I0402 20:29:57.890700   56533 cruntime.go:279] skipping containerd shutdown because we are bound to it
I0402 20:29:57.890869   56533 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I0402 20:29:57.898834   56533 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I0402 20:29:57.918197   56533 ssh_runner.go:195] Run: which cri-dockerd
I0402 20:29:57.978604   56533 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I0402 20:29:58.001616   56533 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (190 bytes)
I0402 20:29:58.051340   56533 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I0402 20:29:58.187170   56533 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I0402 20:29:58.269852   56533 docker.go:574] configuring docker to use "cgroupfs" as cgroup driver...
I0402 20:29:58.270042   56533 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (130 bytes)
I0402 20:29:58.283525   56533 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0402 20:29:58.341118   56533 ssh_runner.go:195] Run: sudo systemctl restart docker
I0402 20:29:58.625159   56533 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.socket
I0402 20:29:58.634770   56533 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0402 20:29:58.643461   56533 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I0402 20:29:58.701974   56533 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0402 20:29:58.762116   56533 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0402 20:29:58.816808   56533 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I0402 20:29:58.838642   56533 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0402 20:29:58.846592   56533 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0402 20:29:58.901883   56533 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.service
I0402 20:29:59.048380   56533 start.go:542] Will wait 60s for socket path /var/run/cri-dockerd.sock
I0402 20:29:59.049644   56533 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I0402 20:29:59.053060   56533 start.go:563] Will wait 60s for crictl version
I0402 20:29:59.053147   56533 ssh_runner.go:195] Run: which crictl
I0402 20:29:59.056173   56533 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I0402 20:29:59.119194   56533 start.go:579] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  27.4.1
RuntimeApiVersion:  v1
I0402 20:29:59.119404   56533 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0402 20:29:59.196823   56533 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0402 20:29:59.219361   56533 out.go:235] üê≥  Ïø†Î≤ÑÎÑ§Ìã∞Ïä§ v1.32.0 ÏùÑ Docker 27.4.1 Îü∞ÌÉÄÏûÑÏúºÎ°ú ÏÑ§ÏπòÌïòÎäî Ï§ë
I0402 20:29:59.219569   56533 cli_runner.go:164] Run: docker exec -t minikube dig +short host.docker.internal
I0402 20:29:59.473769   56533 network.go:96] got host ip for mount in container by digging dns: 192.168.65.2
I0402 20:29:59.474202   56533 ssh_runner.go:195] Run: grep 192.168.65.2	host.minikube.internal$ /etc/hosts
I0402 20:29:59.477949   56533 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.65.2	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0402 20:29:59.486254   56533 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0402 20:29:59.594592   56533 kubeadm.go:883] updating cluster {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 Memory:4000 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.32.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/Users:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} ...
I0402 20:29:59.594662   56533 preload.go:131] Checking if preload exists for k8s version v1.32.0 and runtime docker
I0402 20:29:59.594740   56533 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0402 20:29:59.609151   56533 docker.go:689] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.32.0
registry.k8s.io/kube-controller-manager:v1.32.0
registry.k8s.io/kube-scheduler:v1.32.0
registry.k8s.io/kube-proxy:v1.32.0
registry.k8s.io/etcd:3.5.16-0
registry.k8s.io/coredns/coredns:v1.11.3
registry.k8s.io/pause:3.10
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0402 20:29:59.609163   56533 docker.go:619] Images already preloaded, skipping extraction
I0402 20:29:59.609519   56533 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0402 20:29:59.623333   56533 docker.go:689] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.32.0
registry.k8s.io/kube-controller-manager:v1.32.0
registry.k8s.io/kube-scheduler:v1.32.0
registry.k8s.io/kube-proxy:v1.32.0
registry.k8s.io/etcd:3.5.16-0
registry.k8s.io/coredns/coredns:v1.11.3
registry.k8s.io/pause:3.10
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0402 20:29:59.623345   56533 cache_images.go:84] Images are preloaded, skipping loading
I0402 20:29:59.623352   56533 kubeadm.go:934] updating node { 192.168.49.2 8443 v1.32.0 docker true true} ...
I0402 20:29:59.623673   56533 kubeadm.go:946] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.32.0/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.2

[Install]
 config:
{KubernetesVersion:v1.32.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:}
I0402 20:29:59.623779   56533 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I0402 20:29:59.721418   56533 cni.go:84] Creating CNI manager for ""
I0402 20:29:59.721434   56533 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0402 20:29:59.721459   56533 kubeadm.go:84] Using pod CIDR: 10.244.0.0/16
I0402 20:29:59.721480   56533 kubeadm.go:189] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.2 APIServerPort:8443 KubernetesVersion:v1.32.0 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.49.2 CgroupDriver:cgroupfs ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[containerRuntimeEndpoint:unix:///var/run/cri-dockerd.sock hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I0402 20:29:59.721556   56533 kubeadm.go:195] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta4
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    - name: "node-ip"
      value: "192.168.49.2"
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta4
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    - name: "enable-admission-plugins"
      value: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    - name: "allocate-node-cidrs"
      value: "true"
    - name: "leader-elect"
      value: "false"
scheduler:
  extraArgs:
    - name: "leader-elect"
      value: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      - name: "proxy-refresh-interval"
        value: "70000"
kubernetesVersion: v1.32.0
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: cgroupfs
containerRuntimeEndpoint: unix:///var/run/cri-dockerd.sock
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%"
  nodefs.inodesFree: "0%"
  imagefs.available: "0%"
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I0402 20:29:59.721709   56533 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.32.0
I0402 20:29:59.729128   56533 binaries.go:44] Found k8s binaries, skipping transfer
I0402 20:29:59.729245   56533 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I0402 20:29:59.735703   56533 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (307 bytes)
I0402 20:29:59.747413   56533 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I0402 20:29:59.758916   56533 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2286 bytes)
I0402 20:29:59.770663   56533 ssh_runner.go:195] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I0402 20:29:59.774439   56533 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.49.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0402 20:29:59.781640   56533 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0402 20:29:59.849799   56533 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0402 20:29:59.861382   56533 certs.go:68] Setting up /Users/cho-cho/.minikube/profiles/minikube for IP: 192.168.49.2
I0402 20:29:59.861394   56533 certs.go:194] generating shared ca certs ...
I0402 20:29:59.861410   56533 certs.go:226] acquiring lock for ca certs: {Name:mk97cf805a6422c7ab5f07e436dc53a8d1bda6d3 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0402 20:29:59.861621   56533 certs.go:240] generating "minikubeCA" ca cert: /Users/cho-cho/.minikube/ca.key
I0402 20:29:59.983572   56533 crypto.go:156] Writing cert to /Users/cho-cho/.minikube/ca.crt ...
I0402 20:29:59.983579   56533 lock.go:35] WriteFile acquiring /Users/cho-cho/.minikube/ca.crt: {Name:mk8a0f35732643c69dad7ebc5e03600ecb1a033b Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0402 20:29:59.984378   56533 crypto.go:164] Writing key to /Users/cho-cho/.minikube/ca.key ...
I0402 20:29:59.984382   56533 lock.go:35] WriteFile acquiring /Users/cho-cho/.minikube/ca.key: {Name:mk7e5a8fdce6a1295fee2bf407c256c74005c642 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0402 20:29:59.984496   56533 certs.go:240] generating "proxyClientCA" ca cert: /Users/cho-cho/.minikube/proxy-client-ca.key
I0402 20:30:00.102951   56533 crypto.go:156] Writing cert to /Users/cho-cho/.minikube/proxy-client-ca.crt ...
I0402 20:30:00.102960   56533 lock.go:35] WriteFile acquiring /Users/cho-cho/.minikube/proxy-client-ca.crt: {Name:mk8305eef81db59bdfd8d19197b8e559b7f24cc5 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0402 20:30:00.103270   56533 crypto.go:164] Writing key to /Users/cho-cho/.minikube/proxy-client-ca.key ...
I0402 20:30:00.103273   56533 lock.go:35] WriteFile acquiring /Users/cho-cho/.minikube/proxy-client-ca.key: {Name:mk16c1aba055c2fe986c7ba87b9d13677ba0576b Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0402 20:30:00.104491   56533 certs.go:256] generating profile certs ...
I0402 20:30:00.104525   56533 certs.go:363] generating signed profile cert for "minikube-user": /Users/cho-cho/.minikube/profiles/minikube/client.key
I0402 20:30:00.104531   56533 crypto.go:68] Generating cert /Users/cho-cho/.minikube/profiles/minikube/client.crt with IP's: []
I0402 20:30:00.179544   56533 crypto.go:156] Writing cert to /Users/cho-cho/.minikube/profiles/minikube/client.crt ...
I0402 20:30:00.179553   56533 lock.go:35] WriteFile acquiring /Users/cho-cho/.minikube/profiles/minikube/client.crt: {Name:mk6700a1edf44230365357d830e37f2d8c2cdf2f Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0402 20:30:00.179810   56533 crypto.go:164] Writing key to /Users/cho-cho/.minikube/profiles/minikube/client.key ...
I0402 20:30:00.179812   56533 lock.go:35] WriteFile acquiring /Users/cho-cho/.minikube/profiles/minikube/client.key: {Name:mkc6a88043a97b1d7ea0d85b11efdeeaf546a4e5 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0402 20:30:00.179927   56533 certs.go:363] generating signed profile cert for "minikube": /Users/cho-cho/.minikube/profiles/minikube/apiserver.key.7fb57e3c
I0402 20:30:00.179937   56533 crypto.go:68] Generating cert /Users/cho-cho/.minikube/profiles/minikube/apiserver.crt.7fb57e3c with IP's: [10.96.0.1 127.0.0.1 10.0.0.1 192.168.49.2]
I0402 20:30:00.336870   56533 crypto.go:156] Writing cert to /Users/cho-cho/.minikube/profiles/minikube/apiserver.crt.7fb57e3c ...
I0402 20:30:00.336878   56533 lock.go:35] WriteFile acquiring /Users/cho-cho/.minikube/profiles/minikube/apiserver.crt.7fb57e3c: {Name:mkd483501d365dcd5176acab4c5e7a7820f4c257 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0402 20:30:00.337116   56533 crypto.go:164] Writing key to /Users/cho-cho/.minikube/profiles/minikube/apiserver.key.7fb57e3c ...
I0402 20:30:00.337119   56533 lock.go:35] WriteFile acquiring /Users/cho-cho/.minikube/profiles/minikube/apiserver.key.7fb57e3c: {Name:mk3cc7c7bdcf0ea68e8c1119134982b8dd39d253 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0402 20:30:00.337224   56533 certs.go:381] copying /Users/cho-cho/.minikube/profiles/minikube/apiserver.crt.7fb57e3c -> /Users/cho-cho/.minikube/profiles/minikube/apiserver.crt
I0402 20:30:00.337450   56533 certs.go:385] copying /Users/cho-cho/.minikube/profiles/minikube/apiserver.key.7fb57e3c -> /Users/cho-cho/.minikube/profiles/minikube/apiserver.key
I0402 20:30:00.337554   56533 certs.go:363] generating signed profile cert for "aggregator": /Users/cho-cho/.minikube/profiles/minikube/proxy-client.key
I0402 20:30:00.337568   56533 crypto.go:68] Generating cert /Users/cho-cho/.minikube/profiles/minikube/proxy-client.crt with IP's: []
I0402 20:30:00.515614   56533 crypto.go:156] Writing cert to /Users/cho-cho/.minikube/profiles/minikube/proxy-client.crt ...
I0402 20:30:00.515626   56533 lock.go:35] WriteFile acquiring /Users/cho-cho/.minikube/profiles/minikube/proxy-client.crt: {Name:mk6a357ad79b228b4acdd34b32f246343cf8d189 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0402 20:30:00.515909   56533 crypto.go:164] Writing key to /Users/cho-cho/.minikube/profiles/minikube/proxy-client.key ...
I0402 20:30:00.515912   56533 lock.go:35] WriteFile acquiring /Users/cho-cho/.minikube/profiles/minikube/proxy-client.key: {Name:mk4e92b67f7def616bc360d0b092a57fc1e0a2d1 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0402 20:30:00.516215   56533 certs.go:484] found cert: /Users/cho-cho/.minikube/certs/ca-key.pem (1679 bytes)
I0402 20:30:00.516403   56533 certs.go:484] found cert: /Users/cho-cho/.minikube/certs/ca.pem (1082 bytes)
I0402 20:30:00.516442   56533 certs.go:484] found cert: /Users/cho-cho/.minikube/certs/cert.pem (1123 bytes)
I0402 20:30:00.516560   56533 certs.go:484] found cert: /Users/cho-cho/.minikube/certs/key.pem (1675 bytes)
I0402 20:30:00.517958   56533 ssh_runner.go:362] scp /Users/cho-cho/.minikube/ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I0402 20:30:00.550534   56533 ssh_runner.go:362] scp /Users/cho-cho/.minikube/ca.key --> /var/lib/minikube/certs/ca.key (1679 bytes)
I0402 20:30:00.567187   56533 ssh_runner.go:362] scp /Users/cho-cho/.minikube/proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I0402 20:30:00.586069   56533 ssh_runner.go:362] scp /Users/cho-cho/.minikube/proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1679 bytes)
I0402 20:30:00.603880   56533 ssh_runner.go:362] scp /Users/cho-cho/.minikube/profiles/minikube/apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1411 bytes)
I0402 20:30:00.619205   56533 ssh_runner.go:362] scp /Users/cho-cho/.minikube/profiles/minikube/apiserver.key --> /var/lib/minikube/certs/apiserver.key (1679 bytes)
I0402 20:30:00.634637   56533 ssh_runner.go:362] scp /Users/cho-cho/.minikube/profiles/minikube/proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I0402 20:30:00.650324   56533 ssh_runner.go:362] scp /Users/cho-cho/.minikube/profiles/minikube/proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1675 bytes)
I0402 20:30:00.664360   56533 ssh_runner.go:362] scp /Users/cho-cho/.minikube/ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I0402 20:30:00.678614   56533 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)
I0402 20:30:00.690055   56533 ssh_runner.go:195] Run: openssl version
I0402 20:30:00.699012   56533 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I0402 20:30:00.708830   56533 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I0402 20:30:00.713487   56533 certs.go:528] hashing: -rw-r--r-- 1 root root 1111 Apr  2 11:29 /usr/share/ca-certificates/minikubeCA.pem
I0402 20:30:00.713645   56533 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I0402 20:30:00.719950   56533 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I0402 20:30:00.729300   56533 ssh_runner.go:195] Run: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt
I0402 20:30:00.745268   56533 certs.go:399] 'apiserver-kubelet-client' cert doesn't exist, likely first start: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt: Process exited with status 1
stdout:

stderr:
stat: cannot statx '/var/lib/minikube/certs/apiserver-kubelet-client.crt': No such file or directory
I0402 20:30:00.745321   56533 kubeadm.go:392] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 Memory:4000 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.32.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/Users:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0402 20:30:00.745462   56533 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0402 20:30:00.761509   56533 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I0402 20:30:00.769415   56533 ssh_runner.go:195] Run: sudo cp /var/tmp/minikube/kubeadm.yaml.new /var/tmp/minikube/kubeadm.yaml
I0402 20:30:00.776048   56533 kubeadm.go:214] ignoring SystemVerification for kubeadm because of docker driver
I0402 20:30:00.776205   56533 ssh_runner.go:195] Run: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf
I0402 20:30:00.782058   56533 kubeadm.go:155] config check failed, skipping stale config cleanup: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf: Process exited with status 2
stdout:

stderr:
ls: cannot access '/etc/kubernetes/admin.conf': No such file or directory
ls: cannot access '/etc/kubernetes/kubelet.conf': No such file or directory
ls: cannot access '/etc/kubernetes/controller-manager.conf': No such file or directory
ls: cannot access '/etc/kubernetes/scheduler.conf': No such file or directory
I0402 20:30:00.782066   56533 kubeadm.go:157] found existing configuration files:

I0402 20:30:00.782187   56533 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf
I0402 20:30:00.788686   56533 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/admin.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/admin.conf: No such file or directory
I0402 20:30:00.788810   56533 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/admin.conf
I0402 20:30:00.794780   56533 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf
I0402 20:30:00.800553   56533 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/kubelet.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/kubelet.conf: No such file or directory
I0402 20:30:00.800671   56533 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/kubelet.conf
I0402 20:30:00.806495   56533 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf
I0402 20:30:00.812322   56533 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/controller-manager.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/controller-manager.conf: No such file or directory
I0402 20:30:00.812427   56533 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/controller-manager.conf
I0402 20:30:00.818380   56533 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf
I0402 20:30:00.824453   56533 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/scheduler.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/scheduler.conf: No such file or directory
I0402 20:30:00.824573   56533 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/scheduler.conf
I0402 20:30:00.830592   56533 ssh_runner.go:286] Start: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.32.0:$PATH" kubeadm init --config /var/tmp/minikube/kubeadm.yaml  --ignore-preflight-errors=DirAvailable--etc-kubernetes-manifests,DirAvailable--var-lib-minikube,DirAvailable--var-lib-minikube-etcd,FileAvailable--etc-kubernetes-manifests-kube-scheduler.yaml,FileAvailable--etc-kubernetes-manifests-kube-apiserver.yaml,FileAvailable--etc-kubernetes-manifests-kube-controller-manager.yaml,FileAvailable--etc-kubernetes-manifests-etcd.yaml,Port-10250,Swap,NumCPU,Mem,SystemVerification,FileContent--proc-sys-net-bridge-bridge-nf-call-iptables"
I0402 20:30:00.867643   56533 kubeadm.go:310] [init] Using Kubernetes version: v1.32.0
I0402 20:30:00.867707   56533 kubeadm.go:310] [preflight] Running pre-flight checks
I0402 20:30:00.944641   56533 kubeadm.go:310] [preflight] Pulling images required for setting up a Kubernetes cluster
I0402 20:30:00.944828   56533 kubeadm.go:310] [preflight] This might take a minute or two, depending on the speed of your internet connection
I0402 20:30:00.944974   56533 kubeadm.go:310] [preflight] You can also perform this action beforehand using 'kubeadm config images pull'
I0402 20:30:00.954465   56533 kubeadm.go:310] [certs] Using certificateDir folder "/var/lib/minikube/certs"
I0402 20:30:00.963791   56533 out.go:235]     ‚ñ™ Ïù∏Ï¶ùÏÑú Î∞è ÌÇ§Î•º ÏÉùÏÑ±ÌïòÎäî Ï§ë ...
I0402 20:30:00.963976   56533 kubeadm.go:310] [certs] Using existing ca certificate authority
I0402 20:30:00.964095   56533 kubeadm.go:310] [certs] Using existing apiserver certificate and key on disk
I0402 20:30:01.034773   56533 kubeadm.go:310] [certs] Generating "apiserver-kubelet-client" certificate and key
I0402 20:30:01.125241   56533 kubeadm.go:310] [certs] Generating "front-proxy-ca" certificate and key
I0402 20:30:01.325655   56533 kubeadm.go:310] [certs] Generating "front-proxy-client" certificate and key
I0402 20:30:01.517259   56533 kubeadm.go:310] [certs] Generating "etcd/ca" certificate and key
I0402 20:30:01.758774   56533 kubeadm.go:310] [certs] Generating "etcd/server" certificate and key
I0402 20:30:01.758951   56533 kubeadm.go:310] [certs] etcd/server serving cert is signed for DNS names [localhost minikube] and IPs [192.168.49.2 127.0.0.1 ::1]
I0402 20:30:01.917782   56533 kubeadm.go:310] [certs] Generating "etcd/peer" certificate and key
I0402 20:30:01.917973   56533 kubeadm.go:310] [certs] etcd/peer serving cert is signed for DNS names [localhost minikube] and IPs [192.168.49.2 127.0.0.1 ::1]
I0402 20:30:01.943810   56533 kubeadm.go:310] [certs] Generating "etcd/healthcheck-client" certificate and key
I0402 20:30:02.031861   56533 kubeadm.go:310] [certs] Generating "apiserver-etcd-client" certificate and key
I0402 20:30:02.200177   56533 kubeadm.go:310] [certs] Generating "sa" key and public key
I0402 20:30:02.200280   56533 kubeadm.go:310] [kubeconfig] Using kubeconfig folder "/etc/kubernetes"
I0402 20:30:02.266286   56533 kubeadm.go:310] [kubeconfig] Writing "admin.conf" kubeconfig file
I0402 20:30:02.408096   56533 kubeadm.go:310] [kubeconfig] Writing "super-admin.conf" kubeconfig file
I0402 20:30:02.539102   56533 kubeadm.go:310] [kubeconfig] Writing "kubelet.conf" kubeconfig file
I0402 20:30:02.807096   56533 kubeadm.go:310] [kubeconfig] Writing "controller-manager.conf" kubeconfig file
I0402 20:30:02.863546   56533 kubeadm.go:310] [kubeconfig] Writing "scheduler.conf" kubeconfig file
I0402 20:30:02.863821   56533 kubeadm.go:310] [etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"
I0402 20:30:02.865127   56533 kubeadm.go:310] [control-plane] Using manifest folder "/etc/kubernetes/manifests"
I0402 20:30:02.875820   56533 out.go:235]     ‚ñ™ Ïª®Ìä∏Î°§ ÌîåÎ†àÏù∏ÏùÑ Î∂ÄÌåÖÌïòÎäî Ï§ë ...
I0402 20:30:02.875974   56533 kubeadm.go:310] [control-plane] Creating static Pod manifest for "kube-apiserver"
I0402 20:30:02.876066   56533 kubeadm.go:310] [control-plane] Creating static Pod manifest for "kube-controller-manager"
I0402 20:30:02.876166   56533 kubeadm.go:310] [control-plane] Creating static Pod manifest for "kube-scheduler"
I0402 20:30:02.983571   56533 kubeadm.go:310] [kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
I0402 20:30:03.012293   56533 kubeadm.go:310] [kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
I0402 20:30:03.013197   56533 kubeadm.go:310] [kubelet-start] Starting the kubelet
I0402 20:30:03.105674   56533 kubeadm.go:310] [wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests"
I0402 20:30:03.105829   56533 kubeadm.go:310] [kubelet-check] Waiting for a healthy kubelet at http://127.0.0.1:10248/healthz. This can take up to 4m0s
I0402 20:30:03.607710   56533 kubeadm.go:310] [kubelet-check] The kubelet is healthy after 501.805541ms
I0402 20:30:03.607887   56533 kubeadm.go:310] [api-check] Waiting for a healthy API server. This can take up to 4m0s
I0402 20:30:07.109863   56533 kubeadm.go:310] [api-check] The API server is healthy after 3.501529626s
I0402 20:30:07.121870   56533 kubeadm.go:310] [upload-config] Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
I0402 20:30:07.131425   56533 kubeadm.go:310] [kubelet] Creating a ConfigMap "kubelet-config" in namespace kube-system with the configuration for the kubelets in the cluster
I0402 20:30:07.142881   56533 kubeadm.go:310] [upload-certs] Skipping phase. Please see --upload-certs
I0402 20:30:07.143188   56533 kubeadm.go:310] [mark-control-plane] Marking the node minikube as control-plane by adding the labels: [node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers]
I0402 20:30:07.148035   56533 kubeadm.go:310] [bootstrap-token] Using token: 3p0ohf.b9rjtppvupdbdg29
I0402 20:30:07.151100   56533 out.go:235]     ‚ñ™ RBAC Í∑úÏπôÏùÑ Íµ¨ÏÑ±ÌïòÎäî Ï§ë ...
I0402 20:30:07.151268   56533 kubeadm.go:310] [bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
I0402 20:30:07.153067   56533 kubeadm.go:310] [bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to get nodes
I0402 20:30:07.162458   56533 kubeadm.go:310] [bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
I0402 20:30:07.165799   56533 kubeadm.go:310] [bootstrap-token] Configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
I0402 20:30:07.167757   56533 kubeadm.go:310] [bootstrap-token] Configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
I0402 20:30:07.169761   56533 kubeadm.go:310] [bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace
I0402 20:30:07.519106   56533 kubeadm.go:310] [kubelet-finalize] Updating "/etc/kubernetes/kubelet.conf" to point to a rotatable kubelet client certificate and key
I0402 20:30:07.923318   56533 kubeadm.go:310] [addons] Applied essential addon: CoreDNS
I0402 20:30:08.515751   56533 kubeadm.go:310] [addons] Applied essential addon: kube-proxy
I0402 20:30:08.516107   56533 kubeadm.go:310] 
I0402 20:30:08.516200   56533 kubeadm.go:310] Your Kubernetes control-plane has initialized successfully!
I0402 20:30:08.516211   56533 kubeadm.go:310] 
I0402 20:30:08.516398   56533 kubeadm.go:310] To start using your cluster, you need to run the following as a regular user:
I0402 20:30:08.516408   56533 kubeadm.go:310] 
I0402 20:30:08.516458   56533 kubeadm.go:310]   mkdir -p $HOME/.kube
I0402 20:30:08.516557   56533 kubeadm.go:310]   sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
I0402 20:30:08.516633   56533 kubeadm.go:310]   sudo chown $(id -u):$(id -g) $HOME/.kube/config
I0402 20:30:08.516641   56533 kubeadm.go:310] 
I0402 20:30:08.516720   56533 kubeadm.go:310] Alternatively, if you are the root user, you can run:
I0402 20:30:08.516741   56533 kubeadm.go:310] 
I0402 20:30:08.516809   56533 kubeadm.go:310]   export KUBECONFIG=/etc/kubernetes/admin.conf
I0402 20:30:08.516816   56533 kubeadm.go:310] 
I0402 20:30:08.516892   56533 kubeadm.go:310] You should now deploy a pod network to the cluster.
I0402 20:30:08.517033   56533 kubeadm.go:310] Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
I0402 20:30:08.517164   56533 kubeadm.go:310]   https://kubernetes.io/docs/concepts/cluster-administration/addons/
I0402 20:30:08.517170   56533 kubeadm.go:310] 
I0402 20:30:08.517279   56533 kubeadm.go:310] You can now join any number of control-plane nodes by copying certificate authorities
I0402 20:30:08.517388   56533 kubeadm.go:310] and service account keys on each node and then running the following as root:
I0402 20:30:08.517391   56533 kubeadm.go:310] 
I0402 20:30:08.517509   56533 kubeadm.go:310]   kubeadm join control-plane.minikube.internal:8443 --token 3p0ohf.b9rjtppvupdbdg29 \
I0402 20:30:08.517646   56533 kubeadm.go:310] 	--discovery-token-ca-cert-hash sha256:ebef34397d90d26529f21f36b5c100b128e301b2e0ed519cbc8a7a1d4ad296e2 \
I0402 20:30:08.517675   56533 kubeadm.go:310] 	--control-plane 
I0402 20:30:08.517682   56533 kubeadm.go:310] 
I0402 20:30:08.517810   56533 kubeadm.go:310] Then you can join any number of worker nodes by running the following on each as root:
I0402 20:30:08.517814   56533 kubeadm.go:310] 
I0402 20:30:08.517925   56533 kubeadm.go:310] kubeadm join control-plane.minikube.internal:8443 --token 3p0ohf.b9rjtppvupdbdg29 \
I0402 20:30:08.518088   56533 kubeadm.go:310] 	--discovery-token-ca-cert-hash sha256:ebef34397d90d26529f21f36b5c100b128e301b2e0ed519cbc8a7a1d4ad296e2 
I0402 20:30:08.524913   56533 kubeadm.go:310] 	[WARNING Swap]: swap is supported for cgroup v2 only. The kubelet must be properly configured to use swap. Please refer to https://kubernetes.io/docs/concepts/architecture/nodes/#swap-memory, or disable swap on the node
I0402 20:30:08.525113   56533 kubeadm.go:310] 	[WARNING Service-Kubelet]: kubelet service is not enabled, please run 'systemctl enable kubelet.service'
I0402 20:30:08.525156   56533 cni.go:84] Creating CNI manager for ""
I0402 20:30:08.525182   56533 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0402 20:30:08.531064   56533 out.go:177] üîó  bridge CNI (Container Networking Interface) Î•º Íµ¨ÏÑ±ÌïòÎäî Ï§ë ...
I0402 20:30:08.538743   56533 ssh_runner.go:195] Run: sudo mkdir -p /etc/cni/net.d
I0402 20:30:08.551401   56533 ssh_runner.go:362] scp memory --> /etc/cni/net.d/1-k8s.conflist (496 bytes)
I0402 20:30:08.565172   56533 ssh_runner.go:195] Run: /bin/bash -c "cat /proc/$(pgrep kube-apiserver)/oom_adj"
I0402 20:30:08.565492   56533 ssh_runner.go:195] Run: sudo /var/lib/minikube/binaries/v1.32.0/kubectl create clusterrolebinding minikube-rbac --clusterrole=cluster-admin --serviceaccount=kube-system:default --kubeconfig=/var/lib/minikube/kubeconfig
I0402 20:30:08.565660   56533 ssh_runner.go:195] Run: sudo /var/lib/minikube/binaries/v1.32.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig label --overwrite nodes minikube minikube.k8s.io/updated_at=2025_04_02T20_30_08_0700 minikube.k8s.io/version=v1.35.0 minikube.k8s.io/commit=dd5d320e41b5451cdf3c01891bc4e13d189586ed minikube.k8s.io/name=minikube minikube.k8s.io/primary=true
I0402 20:30:08.676718   56533 ops.go:34] apiserver oom_adj: -16
I0402 20:30:08.676740   56533 kubeadm.go:1113] duration metric: took 111.553916ms to wait for elevateKubeSystemPrivileges
I0402 20:30:08.719111   56533 kubeadm.go:394] duration metric: took 7.974167084s to StartCluster
I0402 20:30:08.719153   56533 settings.go:142] acquiring lock: {Name:mk4a4a28e754ce1399985c2fb0c58cf95da14281 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0402 20:30:08.719345   56533 settings.go:150] Updating kubeconfig:  /Users/cho-cho/.kube/config
I0402 20:30:08.722151   56533 lock.go:35] WriteFile acquiring /Users/cho-cho/.kube/config: {Name:mkec84c9f0918d30e8749c5401884af853be8f9a Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0402 20:30:08.723409   56533 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.32.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml"
I0402 20:30:08.723674   56533 start.go:235] Will wait 6m0s for node &{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}
I0402 20:30:08.723672   56533 addons.go:511] enable addons start: toEnable=map[ambassador:false amd-gpu-device-plugin:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:false default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false storage-provisioner-rancher:false volcano:false volumesnapshots:false yakd:false]
I0402 20:30:08.723791   56533 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.32.0
I0402 20:30:08.723822   56533 addons.go:69] Setting storage-provisioner=true in profile "minikube"
I0402 20:30:08.723839   56533 addons.go:238] Setting addon storage-provisioner=true in "minikube"
I0402 20:30:08.723895   56533 addons.go:69] Setting default-storageclass=true in profile "minikube"
I0402 20:30:08.723913   56533 host.go:66] Checking if "minikube" exists ...
I0402 20:30:08.723911   56533 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I0402 20:30:08.724997   56533 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0402 20:30:08.725189   56533 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0402 20:30:08.732225   56533 out.go:177] üîé  Kubernetes Íµ¨ÏÑ± ÏöîÏÜåÎ•º ÌôïÏù∏...
I0402 20:30:08.737842   56533 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0402 20:30:08.887066   56533 out.go:177]     ‚ñ™ Using image gcr.io/k8s-minikube/storage-provisioner:v5
I0402 20:30:08.890273   56533 addons.go:435] installing /etc/kubernetes/addons/storage-provisioner.yaml
I0402 20:30:08.890279   56533 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I0402 20:30:08.890384   56533 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0402 20:30:08.891750   56533 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0402 20:30:08.895144   56533 addons.go:238] Setting addon default-storageclass=true in "minikube"
I0402 20:30:08.895201   56533 host.go:66] Checking if "minikube" exists ...
I0402 20:30:08.895576   56533 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0402 20:30:08.998940   56533 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.32.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml | sed -e '/^        forward . \/etc\/resolv.conf.*/i \        hosts {\n           192.168.65.2 host.minikube.internal\n           fallthrough\n        }' -e '/^        errors *$/i \        log' | sudo /var/lib/minikube/binaries/v1.32.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig replace -f -"
I0402 20:30:08.998948   56533 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0402 20:30:09.038445   56533 addons.go:435] installing /etc/kubernetes/addons/storageclass.yaml
I0402 20:30:09.038461   56533 ssh_runner.go:362] scp storageclass/storageclass.yaml --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I0402 20:30:09.038586   56533 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0402 20:30:09.039546   56533 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:62319 SSHKeyPath:/Users/cho-cho/.minikube/machines/minikube/id_rsa Username:docker}
I0402 20:30:09.157605   56533 api_server.go:52] waiting for apiserver process to appear ...
I0402 20:30:09.157691   56533 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0402 20:30:09.192763   56533 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:62319 SSHKeyPath:/Users/cho-cho/.minikube/machines/minikube/id_rsa Username:docker}
I0402 20:30:09.251186   56533 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I0402 20:30:09.285028   56533 start.go:971] {"host.minikube.internal": 192.168.65.2} host record injected into CoreDNS's ConfigMap
I0402 20:30:09.285045   56533 api_server.go:72] duration metric: took 561.364209ms to wait for apiserver process to appear ...
I0402 20:30:09.285052   56533 api_server.go:88] waiting for apiserver healthz status ...
I0402 20:30:09.285064   56533 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:62318/healthz ...
I0402 20:30:09.292510   56533 api_server.go:279] https://127.0.0.1:62318/healthz returned 200:
ok
I0402 20:30:09.302718   56533 api_server.go:141] control plane version: v1.32.0
I0402 20:30:09.302728   56533 api_server.go:131] duration metric: took 17.674375ms to wait for apiserver health ...
I0402 20:30:09.302984   56533 system_pods.go:43] waiting for kube-system pods to appear ...
I0402 20:30:09.311760   56533 system_pods.go:59] 4 kube-system pods found
I0402 20:30:09.311774   56533 system_pods.go:61] "etcd-minikube" [e51db876-3d5f-4dc1-92d8-fcae30cdb724] Running / Ready:ContainersNotReady (containers with unready status: [etcd]) / ContainersReady:ContainersNotReady (containers with unready status: [etcd])
I0402 20:30:09.311781   56533 system_pods.go:61] "kube-apiserver-minikube" [3c5b3046-165d-49f2-8e3d-833da468cce9] Running / Ready:ContainersNotReady (containers with unready status: [kube-apiserver]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-apiserver])
I0402 20:30:09.311786   56533 system_pods.go:61] "kube-controller-manager-minikube" [82812a4d-2969-44dd-ad17-909978164af0] Running / Ready:ContainersNotReady (containers with unready status: [kube-controller-manager]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-controller-manager])
I0402 20:30:09.311791   56533 system_pods.go:61] "kube-scheduler-minikube" [2f92e317-b8a4-4faf-8897-8c409e24c983] Running / Ready:ContainersNotReady (containers with unready status: [kube-scheduler]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-scheduler])
I0402 20:30:09.311794   56533 system_pods.go:74] duration metric: took 8.806708ms to wait for pod list to return data ...
I0402 20:30:09.311801   56533 kubeadm.go:582] duration metric: took 588.122292ms to wait for: map[apiserver:true system_pods:true]
I0402 20:30:09.311808   56533 node_conditions.go:102] verifying NodePressure condition ...
I0402 20:30:09.315335   56533 node_conditions.go:122] node storage ephemeral capacity is 24638800Ki
I0402 20:30:09.315347   56533 node_conditions.go:123] node cpu capacity is 4
I0402 20:30:09.315357   56533 node_conditions.go:105] duration metric: took 3.547625ms to run NodePressure ...
I0402 20:30:09.315363   56533 start.go:241] waiting for startup goroutines ...
I0402 20:30:09.317861   56533 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I0402 20:30:09.473066   56533 out.go:177] üåü  Ïï†ÎìúÏò® ÌôúÏÑ±Ìôî : storage-provisioner, default-storageclass
I0402 20:30:09.483125   56533 addons.go:514] duration metric: took 759.481875ms for enable addons: enabled=[storage-provisioner default-storageclass]
I0402 20:30:09.789468   56533 kapi.go:214] "coredns" deployment in "kube-system" namespace and "minikube" context rescaled to 1 replicas
I0402 20:30:09.789498   56533 start.go:246] waiting for cluster config update ...
I0402 20:30:09.789525   56533 start.go:255] writing updated cluster config ...
I0402 20:30:09.790596   56533 ssh_runner.go:195] Run: rm -f paused
I0402 20:30:09.935247   56533 start.go:600] kubectl: 1.32.3, cluster: 1.32.0 (minor skew: 0)
I0402 20:30:09.940200   56533 out.go:177] üèÑ  ÎÅùÎÇ¨ÏäµÎãàÎã§! kubectlÏù¥ "minikube" ÌÅ¥Îü¨Ïä§ÌÑ∞ÏôÄ "default" ÎÑ§ÏûÑÏä§ÌéòÏù¥Ïä§Î•º Í∏∞Î≥∏Ï†ÅÏúºÎ°ú ÏÇ¨Ïö©ÌïòÎèÑÎ°ù Íµ¨ÏÑ±ÎêòÏóàÏäµÎãàÎã§.


==> Docker <==
Apr 03 05:51:10 minikube cri-dockerd[1519]: time="2025-04-03T05:51:10Z" level=info msg="Stop pulling image docker.io/kubernetesui/dashboard:v2.7.0@sha256:2e500d29e9d5f4a086b908eb8dfe7ecac57d2ab09d65b24f588b1d449841ef93: e5f5e14d8b04: Extracting [==================================================>]  74.08MB/74.08MB"
Apr 03 05:51:32 minikube cri-dockerd[1519]: time="2025-04-03T05:51:32Z" level=info msg="Stop pulling image mongo:latest: Status: Image is up to date for mongo:latest"
Apr 03 05:51:32 minikube dockerd[1242]: time="2025-04-03T05:51:32.499604346Z" level=info msg="ignoring event" container=f6c8d5760f7c7a69a3a3d523a3b2ade6725a354f66d5d4957536ad027e32fea2 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Apr 03 05:52:26 minikube dockerd[1242]: time="2025-04-03T05:52:26.714933177Z" level=info msg="ignoring event" container=8aa4afc14f0045dbaeb2803af590096229548cbaa926cdf60b186f0eb6501aa9 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Apr 03 05:55:32 minikube cri-dockerd[1519]: time="2025-04-03T05:55:32Z" level=info msg="Stop pulling image mongo-express:latest: Status: Image is up to date for mongo-express:latest"
Apr 03 05:56:00 minikube dockerd[1242]: time="2025-04-03T05:56:00.422951595Z" level=info msg="ignoring event" container=d8c526b82c8dfb1a9a1adf033037079680b3fe6e197870f75fd8f3d603534f38 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Apr 03 05:56:03 minikube cri-dockerd[1519]: time="2025-04-03T05:56:03Z" level=error msg="error getting RW layer size for container ID '57a75ef572dfbcd124a3978d9080c7d61aa63fb5c9b0abe1af5ee039acb22cb8': Error response from daemon: No such container: 57a75ef572dfbcd124a3978d9080c7d61aa63fb5c9b0abe1af5ee039acb22cb8"
Apr 03 05:56:03 minikube cri-dockerd[1519]: time="2025-04-03T05:56:03Z" level=error msg="Set backoffDuration to : 1m0s for container ID '57a75ef572dfbcd124a3978d9080c7d61aa63fb5c9b0abe1af5ee039acb22cb8'"
Apr 03 05:56:04 minikube dockerd[1242]: time="2025-04-03T05:56:04.367789639Z" level=info msg="ignoring event" container=98ac644694450941e61c97517bad84aef1293aca3d08f6340b2f47367e9964bc module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Apr 03 05:56:12 minikube dockerd[1242]: time="2025-04-03T05:56:12.113013795Z" level=info msg="ignoring event" container=78ce93a0409f83c691782a1b9a6a63f5f89036ad9c6d2358ec1a10731a89496a module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Apr 03 05:56:19 minikube dockerd[1242]: time="2025-04-03T05:56:19.914205965Z" level=warning msg="reference for unknown type: " digest="sha256:2e500d29e9d5f4a086b908eb8dfe7ecac57d2ab09d65b24f588b1d449841ef93" remote="docker.io/kubernetesui/dashboard@sha256:2e500d29e9d5f4a086b908eb8dfe7ecac57d2ab09d65b24f588b1d449841ef93"
Apr 03 05:56:25 minikube dockerd[1242]: time="2025-04-03T05:56:25.857520385Z" level=info msg="Attempting next endpoint for pull after error: failed to register layer: write /public/fr/Roboto-BoldItalic.e26ba339b06f09f7.woff: no space left on device"
Apr 03 05:56:25 minikube cri-dockerd[1519]: time="2025-04-03T05:56:25Z" level=info msg="Stop pulling image docker.io/kubernetesui/dashboard:v2.7.0@sha256:2e500d29e9d5f4a086b908eb8dfe7ecac57d2ab09d65b24f588b1d449841ef93: e5f5e14d8b04: Extracting [==================================================>]  74.08MB/74.08MB"
Apr 03 05:56:37 minikube cri-dockerd[1519]: time="2025-04-03T05:56:37Z" level=info msg="Stop pulling image mongo:latest: Status: Image is up to date for mongo:latest"
Apr 03 05:56:37 minikube dockerd[1242]: time="2025-04-03T05:56:37.475103209Z" level=info msg="ignoring event" container=8e1413217944dca205bdc22223423cf8b8f5d383ff625c78e8341c13e63355cf module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Apr 03 06:29:16 minikube dockerd[1242]: time="2025-04-03T06:29:16.768353875Z" level=info msg="ignoring event" container=3631d24a9b7e0c81e93df33a7676cb80d6e37e2c6c749c7fe0050ae33e69b502 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Apr 03 06:29:21 minikube cri-dockerd[1519]: time="2025-04-03T06:29:21Z" level=info msg="Stop pulling image mongo:latest: Status: Image is up to date for mongo:latest"
Apr 03 06:29:22 minikube dockerd[1242]: time="2025-04-03T06:29:22.093607711Z" level=info msg="ignoring event" container=7d3d2e8fc225380af71c71aa27e4deff2caa3d872ada0ce7d1f6ccad4b3057c5 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Apr 03 06:29:26 minikube dockerd[1242]: time="2025-04-03T06:29:26.667947880Z" level=info msg="ignoring event" container=d6a269aa1e238ef9f65da69f25943dc8855f57710f507656561516eacb911c75 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Apr 03 06:29:28 minikube cri-dockerd[1519]: time="2025-04-03T06:29:28Z" level=info msg="Stop pulling image mongo-express:latest: Status: Image is up to date for mongo-express:latest"
Apr 03 06:29:39 minikube dockerd[1242]: time="2025-04-03T06:29:39.562025969Z" level=info msg="ignoring event" container=fb1c26f64615b36cdd73624daf1946ca9b578f827fbae6d380b2149b76bb4e11 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Apr 03 06:30:08 minikube dockerd[1242]: time="2025-04-03T06:30:08.749024844Z" level=info msg="ignoring event" container=dd8b3129d9b16ecc814e820bfb69d7132870de731cec91d8d77d74034066c191 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Apr 03 07:07:14 minikube dockerd[1242]: time="2025-04-03T07:07:14.940293542Z" level=info msg="ignoring event" container=94558f7d60646bb63250d1186953453fde620836f927c811f84292868d7cfb26 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Apr 03 07:07:19 minikube dockerd[1242]: time="2025-04-03T07:07:19.803267544Z" level=warning msg="Error getting v2 registry: Get \"https://registry-1.docker.io/v2/\": dial tcp: lookup registry-1.docker.io on 192.168.65.2:53: no such host"
Apr 03 07:07:19 minikube dockerd[1242]: time="2025-04-03T07:07:19.803364669Z" level=info msg="Attempting next endpoint for pull after error: Get \"https://registry-1.docker.io/v2/\": dial tcp: lookup registry-1.docker.io on 192.168.65.2:53: no such host"
Apr 03 07:07:19 minikube dockerd[1242]: time="2025-04-03T07:07:19.806904336Z" level=error msg="Handler for POST /v1.43/images/create returned error: Get \"https://registry-1.docker.io/v2/\": dial tcp: lookup registry-1.docker.io on 192.168.65.2:53: no such host"
Apr 03 07:07:27 minikube dockerd[1242]: time="2025-04-03T07:07:27.108825465Z" level=info msg="ignoring event" container=d9881a8118068d9a5d0a09d00db583268c7e731f43cf040cbc183d980affca68 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Apr 03 07:07:28 minikube cri-dockerd[1519]: time="2025-04-03T07:07:28Z" level=info msg="Stop pulling image mongo-express:latest: Status: Image is up to date for mongo-express:latest"
Apr 03 07:07:34 minikube cri-dockerd[1519]: time="2025-04-03T07:07:34Z" level=info msg="Stop pulling image mongo:latest: Status: Image is up to date for mongo:latest"
Apr 03 07:07:34 minikube dockerd[1242]: time="2025-04-03T07:07:34.862220593Z" level=info msg="ignoring event" container=073ff64f29be9d9e0c3112faedab7069f688ded75316aca88bd3d3c650f1fedc module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Apr 03 07:08:36 minikube dockerd[1242]: time="2025-04-03T07:08:36.605397879Z" level=info msg="ignoring event" container=c5cf0bdb46997767c233cfbe1be10e48eb43c6c2f371d24266c653a56aef3e51 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Apr 03 07:08:37 minikube cri-dockerd[1519]: time="2025-04-03T07:08:37Z" level=error msg="error getting RW layer size for container ID 'fb1c26f64615b36cdd73624daf1946ca9b578f827fbae6d380b2149b76bb4e11': Error response from daemon: No such container: fb1c26f64615b36cdd73624daf1946ca9b578f827fbae6d380b2149b76bb4e11"
Apr 03 07:08:37 minikube cri-dockerd[1519]: time="2025-04-03T07:08:37Z" level=error msg="Set backoffDuration to : 1m0s for container ID 'fb1c26f64615b36cdd73624daf1946ca9b578f827fbae6d380b2149b76bb4e11'"
Apr 03 07:08:52 minikube dockerd[1242]: time="2025-04-03T07:08:52.568987137Z" level=info msg="ignoring event" container=73178da15bdd93d65f69a8062bcbf9c760611c655f3a9aab8d9e1c7039a7059a module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Apr 03 07:11:36 minikube dockerd[1242]: time="2025-04-03T07:11:36.918270554Z" level=warning msg="reference for unknown type: " digest="sha256:2e500d29e9d5f4a086b908eb8dfe7ecac57d2ab09d65b24f588b1d449841ef93" remote="docker.io/kubernetesui/dashboard@sha256:2e500d29e9d5f4a086b908eb8dfe7ecac57d2ab09d65b24f588b1d449841ef93"
Apr 03 07:11:42 minikube dockerd[1242]: time="2025-04-03T07:11:42.903494084Z" level=info msg="Attempting next endpoint for pull after error: failed to register layer: write /public/fr/Roboto-BlackItalic.c8dc642467cb3099.woff: no space left on device"
Apr 03 07:11:42 minikube cri-dockerd[1519]: time="2025-04-03T07:11:42Z" level=info msg="Stop pulling image docker.io/kubernetesui/dashboard:v2.7.0@sha256:2e500d29e9d5f4a086b908eb8dfe7ecac57d2ab09d65b24f588b1d449841ef93: e5f5e14d8b04: Extracting [==================================================>]  74.08MB/74.08MB"
Apr 03 07:12:20 minikube dockerd[1242]: time="2025-04-03T07:12:20.491352546Z" level=info msg="ignoring event" container=74aed8e7e90ccb1660c8fe09c5627358e8f8dab020437b3ec8970292aa8ff664 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Apr 03 07:12:28 minikube dockerd[1242]: time="2025-04-03T07:12:28.999368300Z" level=info msg="ignoring event" container=fc31c167bae05867c217413c8c12479036918d027eefe9a12b0219ab1973428f module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Apr 03 07:12:49 minikube cri-dockerd[1519]: time="2025-04-03T07:12:49Z" level=info msg="Stop pulling image mongo:latest: Status: Image is up to date for mongo:latest"
Apr 03 07:12:49 minikube dockerd[1242]: time="2025-04-03T07:12:49.562248754Z" level=info msg="ignoring event" container=980e90cdef1e8870191166a413e461a425e67a74faadd8b20ee7b83ea03a85d1 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Apr 03 07:13:58 minikube cri-dockerd[1519]: time="2025-04-03T07:13:58Z" level=info msg="Stop pulling image mongo-express:latest: Status: Image is up to date for mongo-express:latest"
Apr 03 07:14:15 minikube dockerd[1242]: time="2025-04-03T07:14:15.125985918Z" level=info msg="ignoring event" container=7f73f7cc5941cf374d92ee4930ec6f12393ddf498aed88c58f9a437c5987cb57 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Apr 03 07:14:38 minikube dockerd[1242]: time="2025-04-03T07:14:38.233169054Z" level=info msg="ignoring event" container=0d02bc0cbc213d4da40ad8a4894ad84508913d51d9df876d845608e25d4db919 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Apr 03 07:14:55 minikube dockerd[1242]: time="2025-04-03T07:14:55.306872840Z" level=info msg="ignoring event" container=3cb6fa6f9e185ca77de8fe1be469af9e4fcbea2c2ad8237454ffa9db1491ce98 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Apr 03 07:16:54 minikube dockerd[1242]: time="2025-04-03T07:16:54.875681548Z" level=warning msg="reference for unknown type: " digest="sha256:2e500d29e9d5f4a086b908eb8dfe7ecac57d2ab09d65b24f588b1d449841ef93" remote="docker.io/kubernetesui/dashboard@sha256:2e500d29e9d5f4a086b908eb8dfe7ecac57d2ab09d65b24f588b1d449841ef93"
Apr 03 07:17:00 minikube dockerd[1242]: time="2025-04-03T07:17:00.773375551Z" level=info msg="Attempting next endpoint for pull after error: failed to register layer: write /public/fr/Roboto-Black.2eaa390d458c877d.woff2: no space left on device"
Apr 03 07:17:00 minikube cri-dockerd[1519]: time="2025-04-03T07:17:00Z" level=info msg="Stop pulling image docker.io/kubernetesui/dashboard:v2.7.0@sha256:2e500d29e9d5f4a086b908eb8dfe7ecac57d2ab09d65b24f588b1d449841ef93: e5f5e14d8b04: Extracting [==================================================>]  74.08MB/74.08MB"
Apr 03 07:17:27 minikube dockerd[1242]: time="2025-04-03T07:17:27.435691341Z" level=info msg="ignoring event" container=1d9bef314f7cae0149ccac75fbf6d15b94039e631e8733a7b3706f1770053ee2 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Apr 03 07:17:28 minikube cri-dockerd[1519]: time="2025-04-03T07:17:28Z" level=error msg="error getting RW layer size for container ID '74aed8e7e90ccb1660c8fe09c5627358e8f8dab020437b3ec8970292aa8ff664': Error response from daemon: No such container: 74aed8e7e90ccb1660c8fe09c5627358e8f8dab020437b3ec8970292aa8ff664"
Apr 03 07:17:28 minikube cri-dockerd[1519]: time="2025-04-03T07:17:28Z" level=error msg="Set backoffDuration to : 1m0s for container ID '74aed8e7e90ccb1660c8fe09c5627358e8f8dab020437b3ec8970292aa8ff664'"
Apr 03 07:17:29 minikube dockerd[1242]: time="2025-04-03T07:17:29.482413758Z" level=info msg="ignoring event" container=94721584dbdefa15ecedc4250ddb31b7697cf0778b77b4137ae5ed072a9ee801 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Apr 03 07:17:59 minikube cri-dockerd[1519]: time="2025-04-03T07:17:59Z" level=info msg="Stop pulling image mongo:latest: Status: Image is up to date for mongo:latest"
Apr 03 07:17:59 minikube dockerd[1242]: time="2025-04-03T07:17:59.417538758Z" level=info msg="ignoring event" container=2181e223bf3b52d02babf7f4796c42119c384f15ba1dce34a1c16571f8751f83 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Apr 03 07:19:49 minikube dockerd[1242]: time="2025-04-03T07:19:49.812767296Z" level=info msg="ignoring event" container=6438ce32b1642faae1707b26688105519121c402e1fe1d83cb742803d2752016 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Apr 03 07:19:51 minikube cri-dockerd[1519]: time="2025-04-03T07:19:51Z" level=info msg="Stop pulling image mongo-express:latest: Status: Image is up to date for mongo-express:latest"
Apr 03 07:20:31 minikube dockerd[1242]: time="2025-04-03T07:20:31.281796509Z" level=info msg="ignoring event" container=4fc09bd33f6b068905cfc8f5f13f1f66ab7163dfc4a820defb490e36d460fa18 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Apr 03 07:22:01 minikube dockerd[1242]: time="2025-04-03T07:22:01.910415468Z" level=warning msg="reference for unknown type: " digest="sha256:2e500d29e9d5f4a086b908eb8dfe7ecac57d2ab09d65b24f588b1d449841ef93" remote="docker.io/kubernetesui/dashboard@sha256:2e500d29e9d5f4a086b908eb8dfe7ecac57d2ab09d65b24f588b1d449841ef93"
Apr 03 07:22:09 minikube dockerd[1242]: time="2025-04-03T07:22:09.565357347Z" level=info msg="Attempting next endpoint for pull after error: failed to register layer: write /public/fr/MaterialIcons-Regular.1e50f5c2ffa6aba4.eot: no space left on device"
Apr 03 07:22:09 minikube cri-dockerd[1519]: time="2025-04-03T07:22:09Z" level=info msg="Stop pulling image docker.io/kubernetesui/dashboard:v2.7.0@sha256:2e500d29e9d5f4a086b908eb8dfe7ecac57d2ab09d65b24f588b1d449841ef93: e5f5e14d8b04: Extracting [==================================================>]  74.08MB/74.08MB"


==> container status <==
CONTAINER           IMAGE                                                                                                                        CREATED             STATE               NAME                        ATTEMPT             POD ID              POD
4fc09bd33f6b0       mongo-express@sha256:1b23d7976f0210dbec74045c209e52fbb26d29b2e873d6c6fa3d3f0ae32c2a64                                        2 minutes ago       Exited              mongo-express               139                 3c1b933bbf5d9       mongo-express-deployment-5dd87b9fcf-5pjm4
6438ce32b1642       ab2b3b758599a                                                                                                                3 minutes ago       Exited              logstash                    132                 14003ee4de1ae       logstash-deployment-7f495c69df-pq79f
2181e223bf3b5       mongo@sha256:1cb283500219e8fc0b61b328ea5a199a395a753d88b17351c58874fb425223cb                                                4 minutes ago       Exited              mongodb                     133                 57a08f835018d       mongodb-stateful-0
94721584dbdef       5548a49bb60ba                                                                                                                4 minutes ago       Exited              metrics-server              134                 e8f6c11b2690c       metrics-server-7fbb699795-lph7c
1d9bef314f7ca       a422e0e982356                                                                                                                4 minutes ago       Exited              dashboard-metrics-scraper   134                 5b2c5179b25ee       dashboard-metrics-scraper-5d59dccf9b-gnc7p
acea86a3959aa       b56bccaafd7d0                                                                                                                7 minutes ago       Running             kibana                      12                  9a0d40daa676c       kibana-deployment-88ddd6fbd-hljzx
3cb6fa6f9e185       b56bccaafd7d0                                                                                                                2 hours ago         Exited              kibana                      11                  9a0d40daa676c       kibana-deployment-88ddd6fbd-hljzx
52b96f69b3d7f       ba04bb24b9575                                                                                                                16 hours ago        Running             storage-provisioner         3                   c85d89902bd16       storage-provisioner
bfebc580ea364       ba04bb24b9575                                                                                                                16 hours ago        Exited              storage-provisioner         2                   c85d89902bd16       storage-provisioner
a3ba2c8e782fe       docker.elastic.co/elasticsearch/elasticsearch@sha256:a1dce08d504b22e87adc849c94dcae53f6a0bd12648a4d99d7f9fc07bb2e8a3e        20 hours ago        Running             elasticsearch               0                   0ff185af284ac       elasticsearch-deployment-77b7d95759-bmt27
9f853b3e80a8f       doohwancho/fastapi-image-test-k8@sha256:549c7f8371966bcda612f0b8aa0c076404e42410e864bcc1d31189004f8f9fd4                     20 hours ago        Running             fastapi-app-container       0                   6127a0e973b79       fastapi-deployment-956cb4b4b-vxljd
1cfdd9f410bce       doohwancho/fastapi-image-test-k8@sha256:549c7f8371966bcda612f0b8aa0c076404e42410e864bcc1d31189004f8f9fd4                     20 hours ago        Running             fastapi-app-container       0                   ef942572b494a       fastapi-deployment-956cb4b4b-7mfqx
5a9cde76bb1a7       registry.k8s.io/ingress-nginx/controller@sha256:d56f135b6462cfc476447cfe564b83a45e8bb7da2774963b00d12161112270b7             20 hours ago        Running             controller                  0                   ff77d5415620a       ingress-nginx-controller-56d7c84fd4-nrp7h
cd301727b6861       registry.k8s.io/ingress-nginx/kube-webhook-certgen@sha256:a9f03b34a3cbfbb26d103a14046ab2c5130a80c3d69d526ff8063d2b37b9fd3f   20 hours ago        Exited              patch                       0                   71d9580091891       ingress-nginx-admission-patch-6tkdn
1a351bd3adb5a       registry.k8s.io/ingress-nginx/kube-webhook-certgen@sha256:a9f03b34a3cbfbb26d103a14046ab2c5130a80c3d69d526ff8063d2b37b9fd3f   20 hours ago        Exited              create                      0                   3b5fe4dc22397       ingress-nginx-admission-create-fvgfg
f5c6a3a9c166a       2f6c962e7b831                                                                                                                20 hours ago        Running             coredns                     0                   41b9d13d87383       coredns-668d6bf9bc-ln8dv
f9d5d6f619dbf       2f50386e20bfd                                                                                                                20 hours ago        Running             kube-proxy                  0                   12d07a794fa2a       kube-proxy-qb8vj
386ad6b3a24af       a8d049396f6b8                                                                                                                20 hours ago        Running             kube-controller-manager     0                   746134dc0c024       kube-controller-manager-minikube
7de4df582042e       7fc9d4aa817aa                                                                                                                20 hours ago        Running             etcd                        0                   5e326fcdac323       etcd-minikube
0c94e848f573c       c3ff26fb59f37                                                                                                                20 hours ago        Running             kube-scheduler              0                   73042a201bf88       kube-scheduler-minikube
8dcecea7ec1c5       2b5bd0f16085a                                                                                                                20 hours ago        Running             kube-apiserver              0                   cba2dc968f422       kube-apiserver-minikube


==> controller_ingress [5a9cde76bb1a] <==
W0403 04:41:02.715964       7 controller.go:1216] Service "default/mongo-express-service" does not have any active Endpoint.
W0403 04:43:15.374959       7 controller.go:1216] Service "default/mongo-express-service" does not have any active Endpoint.
W0403 04:45:06.001127       7 controller.go:1216] Service "default/mongo-express-service" does not have any active Endpoint.
W0403 04:45:39.805264       7 controller.go:1216] Service "default/mongo-express-service" does not have any active Endpoint.
W0403 04:46:45.434465       7 controller.go:1216] Service "default/mongo-express-service" does not have any active Endpoint.
W0403 04:50:51.264750       7 controller.go:1216] Service "default/mongo-express-service" does not have any active Endpoint.
W0403 04:51:19.550435       7 controller.go:1216] Service "default/mongo-express-service" does not have any active Endpoint.
W0403 04:52:28.670361       7 controller.go:1216] Service "default/mongo-express-service" does not have any active Endpoint.
W0403 04:56:30.941540       7 controller.go:1216] Service "default/mongo-express-service" does not have any active Endpoint.
W0403 04:56:53.090857       7 controller.go:1216] Service "default/mongo-express-service" does not have any active Endpoint.
W0403 04:58:16.063504       7 controller.go:1216] Service "default/mongo-express-service" does not have any active Endpoint.
W0403 05:01:54.175171       7 controller.go:1216] Service "default/mongo-express-service" does not have any active Endpoint.
W0403 05:02:16.424670       7 controller.go:1216] Service "default/mongo-express-service" does not have any active Endpoint.
W0403 05:04:03.941649       7 controller.go:1216] Service "default/mongo-express-service" does not have any active Endpoint.
W0403 05:07:22.605665       7 controller.go:1216] Service "default/mongo-express-service" does not have any active Endpoint.
W0403 05:07:50.878154       7 controller.go:1216] Service "default/mongo-express-service" does not have any active Endpoint.
W0403 05:09:54.829004       7 controller.go:1216] Service "default/mongo-express-service" does not have any active Endpoint.
W0403 05:12:52.254914       7 controller.go:1216] Service "default/mongo-express-service" does not have any active Endpoint.
W0403 05:13:15.474714       7 controller.go:1216] Service "default/mongo-express-service" does not have any active Endpoint.
W0403 05:15:37.811556       7 controller.go:1216] Service "default/mongo-express-service" does not have any active Endpoint.
W0403 05:18:24.529787       7 controller.go:1216] Service "default/mongo-express-service" does not have any active Endpoint.
W0403 05:18:52.829913       7 controller.go:1216] Service "default/mongo-express-service" does not have any active Endpoint.
W0403 05:20:15.455595       7 controller.go:1216] Service "default/mongo-express-service" does not have any active Endpoint.
W0403 05:20:15.455659       7 controller.go:1216] Service "default/kibana-service" does not have any active Endpoint.
W0403 05:20:18.798082       7 controller.go:1216] Service "default/mongo-express-service" does not have any active Endpoint.
W0403 05:20:32.651941       7 controller.go:1216] Service "default/mongo-express-service" does not have any active Endpoint.
W0403 05:20:35.983048       7 controller.go:1216] Service "default/mongo-express-service" does not have any active Endpoint.
W0403 05:21:21.911510       7 controller.go:1216] Service "default/mongo-express-service" does not have any active Endpoint.
W0403 05:23:57.240146       7 controller.go:1216] Service "default/mongo-express-service" does not have any active Endpoint.
W0403 05:24:25.755664       7 controller.go:1216] Service "default/mongo-express-service" does not have any active Endpoint.
W0403 05:27:17.543448       7 controller.go:1216] Service "default/mongo-express-service" does not have any active Endpoint.
W0403 05:29:38.805605       7 controller.go:1216] Service "default/mongo-express-service" does not have any active Endpoint.
W0403 05:30:02.011582       7 controller.go:1216] Service "default/mongo-express-service" does not have any active Endpoint.
W0403 05:33:09.086926       7 controller.go:1216] Service "default/mongo-express-service" does not have any active Endpoint.
W0403 05:35:16.702072       7 controller.go:1216] Service "default/mongo-express-service" does not have any active Endpoint.
W0403 05:35:44.107090       7 controller.go:1216] Service "default/mongo-express-service" does not have any active Endpoint.
W0403 05:38:52.321996       7 controller.go:1216] Service "default/mongo-express-service" does not have any active Endpoint.
W0403 05:40:52.211415       7 controller.go:1216] Service "default/mongo-express-service" does not have any active Endpoint.
W0403 05:41:12.494221       7 controller.go:1216] Service "default/mongo-express-service" does not have any active Endpoint.
W0403 05:41:15.826356       7 controller.go:1216] Service "default/mongo-express-service" does not have any active Endpoint.
W0403 05:41:19.164934       7 controller.go:1216] Service "default/mongo-express-service" does not have any active Endpoint.
W0403 05:44:39.007865       7 controller.go:1216] Service "default/mongo-express-service" does not have any active Endpoint.
W0403 05:46:18.786081       7 controller.go:1216] Service "default/mongo-express-service" does not have any active Endpoint.
W0403 05:46:48.096999       7 controller.go:1216] Service "default/mongo-express-service" does not have any active Endpoint.
W0403 05:50:31.234691       7 controller.go:1216] Service "default/mongo-express-service" does not have any active Endpoint.
W0403 05:52:00.443651       7 controller.go:1216] Service "default/mongo-express-service" does not have any active Endpoint.
W0403 05:52:27.051151       7 controller.go:1216] Service "default/mongo-express-service" does not have any active Endpoint.
W0403 05:56:12.991207       7 controller.go:1216] Service "default/mongo-express-service" does not have any active Endpoint.
W0403 06:29:20.135047       7 controller.go:1216] Service "default/mongo-express-service" does not have any active Endpoint.
W0403 06:30:09.852196       7 controller.go:1216] Service "default/mongo-express-service" does not have any active Endpoint.
W0403 07:07:21.794633       7 controller.go:1216] Service "default/mongo-express-service" does not have any active Endpoint.
W0403 07:08:53.611091       7 controller.go:1216] Service "default/mongo-express-service" does not have any active Endpoint.
W0403 07:13:47.243949       7 controller.go:1216] Service "default/mongo-express-service" does not have any active Endpoint.
W0403 07:14:38.935630       7 controller.go:1216] Service "default/mongo-express-service" does not have any active Endpoint.
W0403 07:14:56.185973       7 controller.go:1216] Service "default/mongo-express-service" does not have any active Endpoint.
W0403 07:14:56.186101       7 controller.go:1216] Service "default/kibana-service" does not have any active Endpoint.
W0403 07:14:59.520642       7 controller.go:1216] Service "default/mongo-express-service" does not have any active Endpoint.
W0403 07:19:22.687426       7 controller.go:1216] Service "default/mongo-express-service" does not have any active Endpoint.
W0403 07:19:49.948046       7 controller.go:1216] Service "default/mongo-express-service" does not have any active Endpoint.
W0403 07:20:32.094104       7 controller.go:1216] Service "default/mongo-express-service" does not have any active Endpoint.


==> coredns [f5c6a3a9c166] <==
[INFO] 10.244.0.8:35728 - 3647 "A IN mongodb-service.cluster.local. udp 47 false 512" NXDOMAIN qr,aa,rd 140 0.000073875s
[INFO] 10.244.0.7:36081 - 37650 "AAAA IN mongodb-service.svc.cluster.local. udp 51 false 512" NXDOMAIN qr,aa,rd 144 0.000103042s
[INFO] 10.244.0.8:35728 - 27449 "AAAA IN mongodb-service.cluster.local. udp 47 false 512" NXDOMAIN qr,aa,rd 140 0.000080792s
[INFO] 10.244.0.7:36902 - 5925 "A IN mongodb-service.cluster.local. udp 47 false 512" NXDOMAIN qr,aa,rd 140 0.000057708s
[INFO] 10.244.0.7:36902 - 22564 "AAAA IN mongodb-service.cluster.local. udp 47 false 512" NXDOMAIN qr,aa,rd 140 0.000090708s
[INFO] 10.244.0.8:52636 - 23474 "A IN mongodb-service. udp 33 false 512" NXDOMAIN qr,rd,ra 33 0.013275125s
[INFO] 10.244.0.7:40445 - 64459 "A IN mongodb-service. udp 33 false 512" NXDOMAIN qr,rd,ra 33 0.012854042s
[INFO] 10.244.0.7:40445 - 55242 "AAAA IN mongodb-service. udp 33 false 512" NXDOMAIN qr,rd,ra 33 0.007375041s
[INFO] 10.244.0.8:52636 - 23731 "AAAA IN mongodb-service. udp 33 false 512" NXDOMAIN qr,rd,ra 33 0.007993167s
[INFO] 10.244.0.11:40262 - 36486 "AAAA IN elasticsearch-service.default.svc.cluster.local. udp 65 false 512" NOERROR qr,aa,rd 158 0.000820375s
[INFO] 10.244.0.11:40262 - 11904 "A IN elasticsearch-service.default.svc.cluster.local. udp 65 false 512" NOERROR qr,aa,rd 128 0.00109125s
[INFO] 10.244.0.11:33945 - 16233 "AAAA IN elasticsearch-service.default.svc.cluster.local. udp 65 false 512" NOERROR qr,aa,rd 158 0.000193834s
[INFO] 10.244.0.11:33945 - 42607 "A IN elasticsearch-service.default.svc.cluster.local. udp 65 false 512" NOERROR qr,aa,rd 128 0.000280791s
[INFO] 10.244.0.8:60478 - 61106 "A IN mongodb-service.default.svc.cluster.local. udp 59 false 512" NXDOMAIN qr,aa,rd 152 0.000497833s
[INFO] 10.244.0.7:39089 - 52143 "A IN mongodb-service.default.svc.cluster.local. udp 59 false 512" NXDOMAIN qr,aa,rd 152 0.000864875s
[INFO] 10.244.0.8:60478 - 47795 "AAAA IN mongodb-service.default.svc.cluster.local. udp 59 false 512" NXDOMAIN qr,aa,rd 152 0.000355708s
[INFO] 10.244.0.7:39089 - 21422 "AAAA IN mongodb-service.default.svc.cluster.local. udp 59 false 512" NXDOMAIN qr,aa,rd 152 0.000095416s
[INFO] 10.244.0.7:41037 - 42732 "A IN mongodb-service.svc.cluster.local. udp 51 false 512" NXDOMAIN qr,aa,rd 144 0.000218s
[INFO] 10.244.0.8:55696 - 62635 "A IN mongodb-service.svc.cluster.local. udp 51 false 512" NXDOMAIN qr,aa,rd 144 0.000234125s
[INFO] 10.244.0.7:41037 - 33514 "AAAA IN mongodb-service.svc.cluster.local. udp 51 false 512" NXDOMAIN qr,aa,rd 144 0.000078459s
[INFO] 10.244.0.7:44636 - 49506 "A IN mongodb-service.cluster.local. udp 47 false 512" NXDOMAIN qr,aa,rd 140 0.000188084s
[INFO] 10.244.0.8:55696 - 18849 "AAAA IN mongodb-service.svc.cluster.local. udp 51 false 512" NXDOMAIN qr,aa,rd 144 0.000339209s
[INFO] 10.244.0.7:44636 - 15392 "AAAA IN mongodb-service.cluster.local. udp 47 false 512" NXDOMAIN qr,aa,rd 140 0.002289s
[INFO] 10.244.0.8:40585 - 36519 "A IN mongodb-service.cluster.local. udp 47 false 512" NXDOMAIN qr,aa,rd 140 0.0001455s
[INFO] 10.244.0.8:40585 - 18852 "AAAA IN mongodb-service.cluster.local. udp 47 false 512" NXDOMAIN qr,aa,rd 140 0.0002745s
[INFO] 10.244.0.7:47153 - 40 "A IN mongodb-service. udp 33 false 512" NXDOMAIN qr,rd,ra 33 0.008295959s
[INFO] 10.244.0.8:48223 - 28132 "A IN mongodb-service. udp 33 false 512" NXDOMAIN qr,rd,ra 33 0.007893834s
[INFO] 10.244.0.7:47153 - 24618 "AAAA IN mongodb-service. udp 33 false 512" NXDOMAIN qr,rd,ra 33 0.010531875s
[INFO] 10.244.0.8:48223 - 57315 "AAAA IN mongodb-service. udp 33 false 512" NXDOMAIN qr,rd,ra 33 0.014716542s
[INFO] 10.244.0.7:38454 - 24657 "A IN mongodb-service.default.svc.cluster.local. udp 59 false 512" NXDOMAIN qr,aa,rd 152 0.028126917s
[INFO] 10.244.0.7:38454 - 1117 "AAAA IN mongodb-service.default.svc.cluster.local. udp 59 false 512" NXDOMAIN qr,aa,rd 152 0.001073791s
[INFO] 10.244.0.7:49993 - 7147 "A IN mongodb-service.svc.cluster.local. udp 51 false 512" NXDOMAIN qr,aa,rd 144 0.001250875s
[INFO] 10.244.0.7:49993 - 8424 "AAAA IN mongodb-service.svc.cluster.local. udp 51 false 512" NXDOMAIN qr,aa,rd 144 0.017196959s
[INFO] 10.244.0.7:42649 - 18899 "A IN mongodb-service.cluster.local. udp 47 false 512" NXDOMAIN qr,aa,rd 140 0.000237459s
[INFO] 10.244.0.7:42649 - 28374 "AAAA IN mongodb-service.cluster.local. udp 47 false 512" NXDOMAIN qr,aa,rd 140 0.000830542s
[INFO] 10.244.0.8:51631 - 14523 "A IN mongodb-service.default.svc.cluster.local. udp 59 false 512" NXDOMAIN qr,aa,rd 152 0.001294417s
[INFO] 10.244.0.8:51631 - 30903 "AAAA IN mongodb-service.default.svc.cluster.local. udp 59 false 512" NXDOMAIN qr,aa,rd 152 0.000494958s
[INFO] 10.244.0.8:35214 - 51693 "A IN mongodb-service.svc.cluster.local. udp 51 false 512" NXDOMAIN qr,aa,rd 144 0.000368917s
[INFO] 10.244.0.8:35214 - 13029 "AAAA IN mongodb-service.svc.cluster.local. udp 51 false 512" NXDOMAIN qr,aa,rd 144 0.005516458s
[INFO] 10.244.0.8:47665 - 7138 "A IN mongodb-service.cluster.local. udp 47 false 512" NXDOMAIN qr,aa,rd 140 0.000189625s
[INFO] 10.244.0.8:47665 - 20204 "AAAA IN mongodb-service.cluster.local. udp 47 false 512" NXDOMAIN qr,aa,rd 140 0.000693458s
[INFO] 10.244.0.7:52584 - 29206 "A IN mongodb-service. udp 33 false 512" NXDOMAIN qr,rd,ra 33 0.044736041s
[INFO] 10.244.0.8:59390 - 38583 "A IN mongodb-service. udp 33 false 512" NXDOMAIN qr,rd,ra 33 0.013948583s
[INFO] 10.244.0.7:52584 - 55827 "AAAA IN mongodb-service. udp 33 false 512" NXDOMAIN qr,rd,ra 33 0.009592917s
[INFO] 10.244.0.8:59390 - 62645 "AAAA IN mongodb-service. udp 33 false 512" NXDOMAIN qr,rd,ra 33 0.007629125s
[INFO] 10.244.0.7:32954 - 49158 "A IN mongodb-service.default.svc.cluster.local. udp 59 false 512" NXDOMAIN qr,aa,rd 152 0.001348667s
[INFO] 10.244.0.8:58886 - 41340 "A IN mongodb-service.default.svc.cluster.local. udp 59 false 512" NXDOMAIN qr,aa,rd 152 0.000738959s
[INFO] 10.244.0.8:58886 - 57465 "AAAA IN mongodb-service.default.svc.cluster.local. udp 59 false 512" NXDOMAIN qr,aa,rd 152 0.000256667s
[INFO] 10.244.0.7:32954 - 64514 "AAAA IN mongodb-service.default.svc.cluster.local. udp 59 false 512" NXDOMAIN qr,aa,rd 152 0.000301333s
[INFO] 10.244.0.8:55377 - 52374 "A IN mongodb-service.svc.cluster.local. udp 51 false 512" NXDOMAIN qr,aa,rd 144 0.000213709s
[INFO] 10.244.0.7:60652 - 56553 "A IN mongodb-service.svc.cluster.local. udp 51 false 512" NXDOMAIN qr,aa,rd 144 0.000333166s
[INFO] 10.244.0.8:55377 - 2963 "AAAA IN mongodb-service.svc.cluster.local. udp 51 false 512" NXDOMAIN qr,aa,rd 144 0.000188542s
[INFO] 10.244.0.7:60652 - 40935 "AAAA IN mongodb-service.svc.cluster.local. udp 51 false 512" NXDOMAIN qr,aa,rd 144 0.000164166s
[INFO] 10.244.0.8:35334 - 42409 "A IN mongodb-service.cluster.local. udp 47 false 512" NXDOMAIN qr,aa,rd 140 0.000167375s
[INFO] 10.244.0.8:35334 - 46756 "AAAA IN mongodb-service.cluster.local. udp 47 false 512" NXDOMAIN qr,aa,rd 140 0.000114292s
[INFO] 10.244.0.7:46632 - 62450 "A IN mongodb-service.cluster.local. udp 47 false 512" NXDOMAIN qr,aa,rd 140 0.000098458s
[INFO] 10.244.0.7:46632 - 26352 "AAAA IN mongodb-service.cluster.local. udp 47 false 512" NXDOMAIN qr,aa,rd 140 0.000120458s
[INFO] 10.244.0.7:41598 - 22732 "A IN mongodb-service. udp 33 false 512" NXDOMAIN qr,rd,ra 33 0.013586792s
[INFO] 10.244.0.8:54746 - 35974 "A IN mongodb-service. udp 33 false 512" NXDOMAIN qr,rd,ra 33 0.014096125s
[INFO] 10.244.0.7:41598 - 11983 "AAAA IN mongodb-service. udp 33 false 512" NXDOMAIN qr,rd,ra 33 0.005307583s


==> describe nodes <==
Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=arm64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=arm64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=dd5d320e41b5451cdf3c01891bc4e13d189586ed
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2025_04_02T20_30_08_0700
                    minikube.k8s.io/version=v1.35.0
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Wed, 02 Apr 2025 11:30:05 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Thu, 03 Apr 2025 07:22:15 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Thu, 03 Apr 2025 07:17:37 +0000   Wed, 02 Apr 2025 11:30:04 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Thu, 03 Apr 2025 07:17:37 +0000   Wed, 02 Apr 2025 11:30:04 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Thu, 03 Apr 2025 07:17:37 +0000   Wed, 02 Apr 2025 11:30:04 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Thu, 03 Apr 2025 07:17:37 +0000   Wed, 02 Apr 2025 11:30:05 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.2
  Hostname:    minikube
Capacity:
  cpu:                4
  ephemeral-storage:  24638800Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  hugepages-32Mi:     0
  hugepages-64Ki:     0
  memory:             8142120Ki
  pods:               110
Allocatable:
  cpu:                4
  ephemeral-storage:  24638800Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  hugepages-32Mi:     0
  hugepages-64Ki:     0
  memory:             8142120Ki
  pods:               110
System Info:
  Machine ID:                 e523ab2a4a634047925c2121ffcde078
  System UUID:                e523ab2a4a634047925c2121ffcde078
  Boot ID:                    d2632d2c-4ef6-43e5-9bb1-f6ab87465b54
  Kernel Version:             5.10.104-linuxkit
  OS Image:                   Ubuntu 22.04.5 LTS
  Operating System:           linux
  Architecture:               arm64
  Container Runtime Version:  docker://27.4.1
  Kubelet Version:            v1.32.0
  Kube-Proxy Version:         v1.32.0
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (18 in total)
  Namespace                   Name                                          CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                          ------------  ----------  ---------------  -------------  ---
  default                     elasticsearch-deployment-77b7d95759-bmt27     100m (2%)     500m (12%)  500Mi (6%)       1Gi (12%)      19h
  default                     fastapi-deployment-956cb4b4b-7mfqx            100m (2%)     500m (12%)  100Mi (1%)       500Mi (6%)     19h
  default                     fastapi-deployment-956cb4b4b-vxljd            100m (2%)     500m (12%)  100Mi (1%)       500Mi (6%)     19h
  default                     kibana-deployment-88ddd6fbd-hljzx             100m (2%)     500m (12%)  1Gi (12%)        1536Mi (19%)   19h
  default                     logstash-deployment-7f495c69df-pq79f          100m (2%)     500m (12%)  1Gi (12%)        1536Mi (19%)   19h
  default                     mongo-express-deployment-5dd87b9fcf-5pjm4     0 (0%)        0 (0%)      0 (0%)           0 (0%)         19h
  default                     mongodb-stateful-0                            0 (0%)        0 (0%)      0 (0%)           0 (0%)         19h
  ingress-nginx               ingress-nginx-controller-56d7c84fd4-nrp7h     100m (2%)     0 (0%)      90Mi (1%)        0 (0%)         19h
  kube-system                 coredns-668d6bf9bc-ln8dv                      100m (2%)     0 (0%)      70Mi (0%)        170Mi (2%)     19h
  kube-system                 etcd-minikube                                 100m (2%)     0 (0%)      100Mi (1%)       0 (0%)         19h
  kube-system                 kube-apiserver-minikube                       250m (6%)     0 (0%)      0 (0%)           0 (0%)         19h
  kube-system                 kube-controller-manager-minikube              200m (5%)     0 (0%)      0 (0%)           0 (0%)         19h
  kube-system                 kube-proxy-qb8vj                              0 (0%)        0 (0%)      0 (0%)           0 (0%)         19h
  kube-system                 kube-scheduler-minikube                       100m (2%)     0 (0%)      0 (0%)           0 (0%)         19h
  kube-system                 metrics-server-7fbb699795-lph7c               100m (2%)     0 (0%)      200Mi (2%)       0 (0%)         19h
  kube-system                 storage-provisioner                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         19h
  kubernetes-dashboard        dashboard-metrics-scraper-5d59dccf9b-gnc7p    0 (0%)        0 (0%)      0 (0%)           0 (0%)         19h
  kubernetes-dashboard        kubernetes-dashboard-7779f9b69b-b5wqm         0 (0%)        0 (0%)      0 (0%)           0 (0%)         19h
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests      Limits
  --------           --------      ------
  cpu                1450m (36%)   2500m (62%)
  memory             3208Mi (40%)  5266Mi (66%)
  ephemeral-storage  0 (0%)        0 (0%)
  hugepages-1Gi      0 (0%)        0 (0%)
  hugepages-2Mi      0 (0%)        0 (0%)
  hugepages-32Mi     0 (0%)        0 (0%)
  hugepages-64Ki     0 (0%)        0 (0%)
Events:              <none>


==> dmesg <==
[Apr 1 22:31] cacheinfo: Unable to detect cache hierarchy for CPU 0
[  +0.032046] the cryptoloop driver has been deprecated and will be removed in in Linux 5.16
[Apr 1 22:32] grpcfuse: loading out-of-tree module taints kernel.
[Apr 1 22:33] tmpfs: Unknown parameter 'noswap'
[  +4.608400] tmpfs: Unknown parameter 'noswap'
[Apr 1 22:41] hrtimer: interrupt took 5985208 ns
[Apr 2 05:24] tmpfs: Unknown parameter 'noswap'
[  +4.426035] tmpfs: Unknown parameter 'noswap'
[Apr 2 06:07] tmpfs: Unknown parameter 'noswap'
[  +4.194281] tmpfs: Unknown parameter 'noswap'
[Apr 2 21:23] tmpfs: Unknown parameter 'noswap'
[  +4.551814] tmpfs: Unknown parameter 'noswap'
[Apr 2 22:14] tmpfs: Unknown parameter 'noswap'
[Apr 2 22:15] tmpfs: Unknown parameter 'noswap'


==> etcd [7de4df582042] <==
{"level":"info","ts":"2025-04-03T04:59:59.708625Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":32196}
{"level":"info","ts":"2025-04-03T04:59:59.715091Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":32196,"took":"4.879375ms","hash":351692760,"current-db-size-bytes":3923968,"current-db-size":"3.9 MB","current-db-size-in-use-bytes":1859584,"current-db-size-in-use":"1.9 MB"}
{"level":"info","ts":"2025-04-03T04:59:59.715137Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":351692760,"revision":32196,"compact-revision":31876}
{"level":"info","ts":"2025-04-03T05:04:59.698375Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":32517}
{"level":"info","ts":"2025-04-03T05:04:59.701688Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":32517,"took":"2.734917ms","hash":412047959,"current-db-size-bytes":3923968,"current-db-size":"3.9 MB","current-db-size-in-use-bytes":1851392,"current-db-size-in-use":"1.9 MB"}
{"level":"info","ts":"2025-04-03T05:04:59.701716Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":412047959,"revision":32517,"compact-revision":32196}
{"level":"info","ts":"2025-04-03T05:09:59.713912Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":32838}
{"level":"info","ts":"2025-04-03T05:09:59.726415Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":32838,"took":"9.39725ms","hash":3540830081,"current-db-size-bytes":3923968,"current-db-size":"3.9 MB","current-db-size-in-use-bytes":1851392,"current-db-size-in-use":"1.9 MB"}
{"level":"info","ts":"2025-04-03T05:09:59.726923Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":3540830081,"revision":32838,"compact-revision":32517}
{"level":"info","ts":"2025-04-03T05:14:59.704572Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":33158}
{"level":"info","ts":"2025-04-03T05:14:59.707869Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":33158,"took":"2.819542ms","hash":2072323626,"current-db-size-bytes":3923968,"current-db-size":"3.9 MB","current-db-size-in-use-bytes":1814528,"current-db-size-in-use":"1.8 MB"}
{"level":"info","ts":"2025-04-03T05:14:59.707897Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":2072323626,"revision":33158,"compact-revision":32838}
{"level":"info","ts":"2025-04-03T05:15:01.936733Z","caller":"etcdserver/server.go:1473","msg":"triggering snapshot","local-member-id":"aec36adc501070cc","local-member-applied-index":40004,"local-member-snapshot-index":30003,"local-member-snapshot-count":10000}
{"level":"info","ts":"2025-04-03T05:15:01.943730Z","caller":"etcdserver/server.go:2493","msg":"saved snapshot","snapshot-index":40004}
{"level":"info","ts":"2025-04-03T05:15:01.944924Z","caller":"etcdserver/server.go:2523","msg":"compacted Raft logs","compact-index":35004}
{"level":"info","ts":"2025-04-03T05:19:59.695061Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":33474}
{"level":"info","ts":"2025-04-03T05:19:59.701170Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":33474,"took":"4.758542ms","hash":2811907103,"current-db-size-bytes":3923968,"current-db-size":"3.9 MB","current-db-size-in-use-bytes":1785856,"current-db-size-in-use":"1.8 MB"}
{"level":"info","ts":"2025-04-03T05:19:59.701219Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":2811907103,"revision":33474,"compact-revision":33158}
{"level":"info","ts":"2025-04-03T05:24:59.829728Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":33788}
{"level":"info","ts":"2025-04-03T05:24:59.837458Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":33788,"took":"7.020666ms","hash":3271183006,"current-db-size-bytes":3923968,"current-db-size":"3.9 MB","current-db-size-in-use-bytes":1863680,"current-db-size-in-use":"1.9 MB"}
{"level":"info","ts":"2025-04-03T05:24:59.837516Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":3271183006,"revision":33788,"compact-revision":33474}
{"level":"info","ts":"2025-04-03T05:29:59.828926Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":34128}
{"level":"info","ts":"2025-04-03T05:29:59.834656Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":34128,"took":"5.322ms","hash":2887496511,"current-db-size-bytes":3923968,"current-db-size":"3.9 MB","current-db-size-in-use-bytes":1912832,"current-db-size-in-use":"1.9 MB"}
{"level":"info","ts":"2025-04-03T05:29:59.834691Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":2887496511,"revision":34128,"compact-revision":33788}
{"level":"info","ts":"2025-04-03T05:34:59.827622Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":34447}
{"level":"info","ts":"2025-04-03T05:34:59.830594Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":34447,"took":"2.5745ms","hash":1078363995,"current-db-size-bytes":3923968,"current-db-size":"3.9 MB","current-db-size-in-use-bytes":1814528,"current-db-size-in-use":"1.8 MB"}
{"level":"info","ts":"2025-04-03T05:34:59.830617Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":1078363995,"revision":34447,"compact-revision":34128}
{"level":"info","ts":"2025-04-03T05:39:59.825622Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":34763}
{"level":"info","ts":"2025-04-03T05:39:59.829321Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":34763,"took":"3.325833ms","hash":2122939084,"current-db-size-bytes":3923968,"current-db-size":"3.9 MB","current-db-size-in-use-bytes":1839104,"current-db-size-in-use":"1.8 MB"}
{"level":"info","ts":"2025-04-03T05:39:59.829344Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":2122939084,"revision":34763,"compact-revision":34447}
{"level":"info","ts":"2025-04-03T05:44:59.823627Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":35086}
{"level":"info","ts":"2025-04-03T05:44:59.826763Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":35086,"took":"2.833333ms","hash":2312053744,"current-db-size-bytes":3923968,"current-db-size":"3.9 MB","current-db-size-in-use-bytes":1884160,"current-db-size-in-use":"1.9 MB"}
{"level":"info","ts":"2025-04-03T05:44:59.826788Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":2312053744,"revision":35086,"compact-revision":34763}
{"level":"info","ts":"2025-04-03T05:49:59.842537Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":35416}
{"level":"info","ts":"2025-04-03T05:49:59.847114Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":35416,"took":"4.118792ms","hash":3149946333,"current-db-size-bytes":3923968,"current-db-size":"3.9 MB","current-db-size-in-use-bytes":1859584,"current-db-size-in-use":"1.9 MB"}
{"level":"info","ts":"2025-04-03T05:49:59.847157Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":3149946333,"revision":35416,"compact-revision":35086}
{"level":"info","ts":"2025-04-03T05:54:59.843242Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":35733}
{"level":"info","ts":"2025-04-03T05:54:59.846365Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":35733,"took":"2.698916ms","hash":2620322830,"current-db-size-bytes":3923968,"current-db-size":"3.9 MB","current-db-size-in-use-bytes":1818624,"current-db-size-in-use":"1.8 MB"}
{"level":"info","ts":"2025-04-03T05:54:59.846388Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":2620322830,"revision":35733,"compact-revision":35416}
{"level":"info","ts":"2025-04-03T07:10:05.855901Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":36051}
{"level":"info","ts":"2025-04-03T07:10:05.861555Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":36051,"took":"5.082584ms","hash":571605167,"current-db-size-bytes":3923968,"current-db-size":"3.9 MB","current-db-size-in-use-bytes":2117632,"current-db-size-in-use":"2.1 MB"}
{"level":"info","ts":"2025-04-03T07:10:05.861630Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":571605167,"revision":36051,"compact-revision":35733}
{"level":"info","ts":"2025-04-03T07:10:37.215777Z","caller":"traceutil/trace.go:171","msg":"trace[1605967053] linearizableReadLoop","detail":"{readStateIndex:43576; appliedIndex:43575; }","duration":"105.993959ms","start":"2025-04-03T07:10:37.109030Z","end":"2025-04-03T07:10:37.215024Z","steps":["trace[1605967053] 'read index received'  (duration: 84.710959ms)","trace[1605967053] 'applied index is now lower than readState.Index'  (duration: 21.281667ms)"],"step_count":2}
{"level":"warn","ts":"2025-04-03T07:10:37.284246Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"165.482417ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/controllerrevisions/\" range_end:\"/registry/controllerrevisions0\" count_only:true ","response":"range_response_count:0 size:8"}
{"level":"info","ts":"2025-04-03T07:10:37.285630Z","caller":"traceutil/trace.go:171","msg":"trace[1817330641] range","detail":"{range_begin:/registry/controllerrevisions/; range_end:/registry/controllerrevisions0; response_count:0; response_revision:36450; }","duration":"176.046126ms","start":"2025-04-03T07:10:37.108997Z","end":"2025-04-03T07:10:37.285043Z","steps":["trace[1817330641] 'agreement among raft nodes before linearized reading'  (duration: 107.841542ms)","trace[1817330641] 'count revisions from in-memory index tree'  (duration: 57.569667ms)"],"step_count":2}
{"level":"info","ts":"2025-04-03T07:15:05.848767Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":36423}
{"level":"info","ts":"2025-04-03T07:15:05.853064Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":36423,"took":"3.510667ms","hash":3901004990,"current-db-size-bytes":3923968,"current-db-size":"3.9 MB","current-db-size-in-use-bytes":2187264,"current-db-size-in-use":"2.2 MB"}
{"level":"info","ts":"2025-04-03T07:15:05.853105Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":3901004990,"revision":36423,"compact-revision":36051}
{"level":"info","ts":"2025-04-03T07:20:05.842534Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":36757}
{"level":"info","ts":"2025-04-03T07:20:05.846194Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":36757,"took":"2.768333ms","hash":258055314,"current-db-size-bytes":3923968,"current-db-size":"3.9 MB","current-db-size-in-use-bytes":1892352,"current-db-size-in-use":"1.9 MB"}
{"level":"info","ts":"2025-04-03T07:20:05.846223Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":258055314,"revision":36757,"compact-revision":36423}
{"level":"info","ts":"2025-04-03T07:20:22.085141Z","caller":"traceutil/trace.go:171","msg":"trace[983754152] linearizableReadLoop","detail":"{readStateIndex:44343; appliedIndex:44341; }","duration":"103.739875ms","start":"2025-04-03T07:20:21.981204Z","end":"2025-04-03T07:20:22.084944Z","steps":["trace[983754152] 'read index received'  (duration: 11.918875ms)","trace[983754152] 'applied index is now lower than readState.Index'  (duration: 91.820375ms)"],"step_count":2}
{"level":"warn","ts":"2025-04-03T07:20:22.096723Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"108.706208ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2025-04-03T07:20:22.096934Z","caller":"traceutil/trace.go:171","msg":"trace[1868477862] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:37084; }","duration":"109.110167ms","start":"2025-04-03T07:20:21.987786Z","end":"2025-04-03T07:20:22.096897Z","steps":["trace[1868477862] 'agreement among raft nodes before linearized reading'  (duration: 108.644375ms)"],"step_count":1}
{"level":"info","ts":"2025-04-03T07:20:22.097978Z","caller":"traceutil/trace.go:171","msg":"trace[367717611] transaction","detail":"{read_only:false; response_revision:37083; number_of_response:1; }","duration":"118.781125ms","start":"2025-04-03T07:20:21.979094Z","end":"2025-04-03T07:20:22.097875Z","steps":["trace[367717611] 'process raft request'  (duration: 104.959083ms)"],"step_count":1}
{"level":"info","ts":"2025-04-03T07:20:22.098249Z","caller":"traceutil/trace.go:171","msg":"trace[2104642329] transaction","detail":"{read_only:false; response_revision:37084; number_of_response:1; }","duration":"116.180583ms","start":"2025-04-03T07:20:21.982048Z","end":"2025-04-03T07:20:22.098229Z","steps":["trace[2104642329] 'process raft request'  (duration: 102.83175ms)"],"step_count":1}
{"level":"warn","ts":"2025-04-03T07:20:22.155664Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"107.410083ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/masterleases/192.168.49.2\" limit:1 ","response":"range_response_count:1 size:136"}
{"level":"info","ts":"2025-04-03T07:20:22.156294Z","caller":"traceutil/trace.go:171","msg":"trace[1552680106] range","detail":"{range_begin:/registry/masterleases/192.168.49.2; range_end:; response_count:1; response_revision:37084; }","duration":"174.881417ms","start":"2025-04-03T07:20:21.981099Z","end":"2025-04-03T07:20:22.155981Z","steps":["trace[1552680106] 'agreement among raft nodes before linearized reading'  (duration: 105.9375ms)"],"step_count":1}
{"level":"warn","ts":"2025-04-03T07:20:22.274580Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"100.215ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128036312431943995 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/masterleases/192.168.49.2\" mod_revision:37076 > success:<request_put:<key:\"/registry/masterleases/192.168.49.2\" value_size:66 lease:8128036312431943993 >> failure:<request_range:<key:\"/registry/masterleases/192.168.49.2\" > >>","response":"size:18"}
{"level":"info","ts":"2025-04-03T07:20:22.274980Z","caller":"traceutil/trace.go:171","msg":"trace[536685811] transaction","detail":"{read_only:false; response_revision:37085; number_of_response:1; }","duration":"104.246542ms","start":"2025-04-03T07:20:22.170686Z","end":"2025-04-03T07:20:22.274932Z","steps":["trace[536685811] 'store kv pair into bolt db' {req_type:put; key:/registry/masterleases/192.168.49.2; req_size:115; } (duration: 99.793125ms)"],"step_count":1}


==> kernel <==
 07:22:23 up 1 day,  8:50,  0 users,  load average: 2.31, 1.46, 0.77
Linux minikube 5.10.104-linuxkit #1 SMP PREEMPT Wed Mar 9 19:01:25 UTC 2022 aarch64 aarch64 aarch64 GNU/Linux
PRETTY_NAME="Ubuntu 22.04.5 LTS"


==> kube-apiserver [8dcecea7ec1c] <==
	, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
 > logger="UnhandledError"
E0403 07:13:15.398455       1 controller.go:113] "Unhandled Error" err="loading OpenAPI spec for \"v1beta1.metrics.k8s.io\" failed with: Error, could not get list of group versions for APIService" logger="UnhandledError"
I0403 07:13:15.399642       1 controller.go:109] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
I0403 07:13:15.399722       1 controller.go:126] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
W0403 07:15:14.394670       1 handler_proxy.go:99] no RequestInfo found in the context
E0403 07:15:14.394972       1 controller.go:146] "Unhandled Error" err=<
	Error updating APIService "v1beta1.metrics.k8s.io" with err: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
	, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
 > logger="UnhandledError"
W0403 07:15:15.398230       1 handler_proxy.go:99] no RequestInfo found in the context
W0403 07:15:15.398262       1 handler_proxy.go:99] no RequestInfo found in the context
E0403 07:15:15.398324       1 controller.go:113] "Unhandled Error" err="loading OpenAPI spec for \"v1beta1.metrics.k8s.io\" failed with: Error, could not get list of group versions for APIService" logger="UnhandledError"
E0403 07:15:15.398363       1 controller.go:102] "Unhandled Error" err=<
	loading OpenAPI spec for "v1beta1.metrics.k8s.io" failed with: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
	, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
 > logger="UnhandledError"
I0403 07:15:15.399461       1 controller.go:109] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
I0403 07:15:15.399477       1 controller.go:126] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
W0403 07:16:15.398140       1 handler_proxy.go:99] no RequestInfo found in the context
W0403 07:16:15.398166       1 handler_proxy.go:99] no RequestInfo found in the context
E0403 07:16:15.398361       1 controller.go:113] "Unhandled Error" err="loading OpenAPI spec for \"v1beta1.metrics.k8s.io\" failed with: Error, could not get list of group versions for APIService" logger="UnhandledError"
E0403 07:16:15.398360       1 controller.go:102] "Unhandled Error" err=<
	loading OpenAPI spec for "v1beta1.metrics.k8s.io" failed with: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
	, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
 > logger="UnhandledError"
I0403 07:16:15.399481       1 controller.go:126] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
I0403 07:16:15.399554       1 controller.go:109] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
W0403 07:18:15.396571       1 handler_proxy.go:99] no RequestInfo found in the context
W0403 07:18:15.396572       1 handler_proxy.go:99] no RequestInfo found in the context
E0403 07:18:15.396827       1 controller.go:113] "Unhandled Error" err="loading OpenAPI spec for \"v1beta1.metrics.k8s.io\" failed with: Error, could not get list of group versions for APIService" logger="UnhandledError"
E0403 07:18:15.396876       1 controller.go:102] "Unhandled Error" err=<
	loading OpenAPI spec for "v1beta1.metrics.k8s.io" failed with: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
	, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
 > logger="UnhandledError"
I0403 07:18:15.398062       1 controller.go:109] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
I0403 07:18:15.398109       1 controller.go:126] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
W0403 07:20:14.398153       1 handler_proxy.go:99] no RequestInfo found in the context
E0403 07:20:14.398560       1 controller.go:146] "Unhandled Error" err=<
	Error updating APIService "v1beta1.metrics.k8s.io" with err: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
	, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
 > logger="UnhandledError"
W0403 07:20:15.431900       1 handler_proxy.go:99] no RequestInfo found in the context
W0403 07:20:15.433027       1 handler_proxy.go:99] no RequestInfo found in the context
E0403 07:20:15.433193       1 controller.go:102] "Unhandled Error" err=<
	loading OpenAPI spec for "v1beta1.metrics.k8s.io" failed with: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
	, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
 > logger="UnhandledError"
E0403 07:20:15.433672       1 controller.go:113] "Unhandled Error" err="loading OpenAPI spec for \"v1beta1.metrics.k8s.io\" failed with: Error, could not get list of group versions for APIService" logger="UnhandledError"
I0403 07:20:15.434392       1 controller.go:109] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
I0403 07:20:15.434848       1 controller.go:126] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
W0403 07:21:15.434273       1 handler_proxy.go:99] no RequestInfo found in the context
W0403 07:21:15.434354       1 handler_proxy.go:99] no RequestInfo found in the context
E0403 07:21:15.435153       1 controller.go:102] "Unhandled Error" err=<
	loading OpenAPI spec for "v1beta1.metrics.k8s.io" failed with: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
	, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
 > logger="UnhandledError"
E0403 07:21:15.435152       1 controller.go:113] "Unhandled Error" err="loading OpenAPI spec for \"v1beta1.metrics.k8s.io\" failed with: Error, could not get list of group versions for APIService" logger="UnhandledError"
I0403 07:21:15.437260       1 controller.go:109] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
I0403 07:21:15.437377       1 controller.go:126] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.


==> kube-controller-manager [386ad6b3a24a] <==
I0403 07:14:15.676432       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/logstash-deployment-7f495c69df" duration="48.541¬µs"
E0403 07:14:23.767332       1 resource_quota_controller.go:446] "Unhandled Error" err="unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: stale GroupVersion discovery: metrics.k8s.io/v1beta1" logger="UnhandledError"
I0403 07:14:23.823303       1 garbagecollector.go:787] "failed to discover some groups" logger="garbage-collector-controller" groups="map[\"metrics.k8s.io/v1beta1\":\"stale GroupVersion discovery: metrics.k8s.io/v1beta1\"]"
I0403 07:14:26.285421       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/logstash-deployment-7f495c69df" duration="106.791¬µs"
I0403 07:14:38.937313       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mongo-express-deployment-5dd87b9fcf" duration="10.0425ms"
I0403 07:14:38.937419       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mongo-express-deployment-5dd87b9fcf" duration="47.542¬µs"
I0403 07:14:53.280834       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mongo-express-deployment-5dd87b9fcf" duration="57.375¬µs"
E0403 07:14:53.776031       1 resource_quota_controller.go:446] "Unhandled Error" err="unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: stale GroupVersion discovery: metrics.k8s.io/v1beta1" logger="UnhandledError"
I0403 07:14:53.863552       1 garbagecollector.go:787] "failed to discover some groups" logger="garbage-collector-controller" groups="map[\"metrics.k8s.io/v1beta1\":\"stale GroupVersion discovery: metrics.k8s.io/v1beta1\"]"
I0403 07:14:56.186105       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/kibana-deployment-88ddd6fbd" duration="17.7795ms"
I0403 07:14:56.187376       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/kibana-deployment-88ddd6fbd" duration="18.209¬µs"
I0403 07:14:57.186150       1 endpointslice_controller.go:344] "Error syncing endpoint slices for service, retrying" logger="endpointslice-controller" key="default/kibana-service" err="EndpointSlice informer cache is out of date"
I0403 07:14:57.187024       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/kibana-deployment-88ddd6fbd" duration="5.592167ms"
I0403 07:14:57.187266       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/kibana-deployment-88ddd6fbd" duration="35.292¬µs"
E0403 07:15:23.783044       1 resource_quota_controller.go:446] "Unhandled Error" err="unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: stale GroupVersion discovery: metrics.k8s.io/v1beta1" logger="UnhandledError"
I0403 07:15:23.869425       1 garbagecollector.go:787] "failed to discover some groups" logger="garbage-collector-controller" groups="map[\"metrics.k8s.io/v1beta1\":\"stale GroupVersion discovery: metrics.k8s.io/v1beta1\"]"
E0403 07:15:53.797176       1 resource_quota_controller.go:446] "Unhandled Error" err="unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: stale GroupVersion discovery: metrics.k8s.io/v1beta1" logger="UnhandledError"
I0403 07:15:53.875706       1 garbagecollector.go:787] "failed to discover some groups" logger="garbage-collector-controller" groups="map[\"metrics.k8s.io/v1beta1\":\"stale GroupVersion discovery: metrics.k8s.io/v1beta1\"]"
E0403 07:16:23.803377       1 resource_quota_controller.go:446] "Unhandled Error" err="unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: stale GroupVersion discovery: metrics.k8s.io/v1beta1" logger="UnhandledError"
I0403 07:16:23.879993       1 garbagecollector.go:787] "failed to discover some groups" logger="garbage-collector-controller" groups="map[\"metrics.k8s.io/v1beta1\":\"stale GroupVersion discovery: metrics.k8s.io/v1beta1\"]"
E0403 07:16:53.815545       1 resource_quota_controller.go:446] "Unhandled Error" err="unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: stale GroupVersion discovery: metrics.k8s.io/v1beta1" logger="UnhandledError"
I0403 07:16:53.890064       1 garbagecollector.go:787] "failed to discover some groups" logger="garbage-collector-controller" groups="map[\"metrics.k8s.io/v1beta1\":\"stale GroupVersion discovery: metrics.k8s.io/v1beta1\"]"
I0403 07:17:14.287012       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-7779f9b69b" duration="102.791¬µs"
E0403 07:17:23.821049       1 resource_quota_controller.go:446] "Unhandled Error" err="unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: stale GroupVersion discovery: metrics.k8s.io/v1beta1" logger="UnhandledError"
I0403 07:17:23.894817       1 garbagecollector.go:787] "failed to discover some groups" logger="garbage-collector-controller" groups="map[\"metrics.k8s.io/v1beta1\":\"stale GroupVersion discovery: metrics.k8s.io/v1beta1\"]"
I0403 07:17:27.481765       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/dashboard-metrics-scraper-5d59dccf9b" duration="51.042¬µs"
I0403 07:17:28.501365       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/dashboard-metrics-scraper-5d59dccf9b" duration="49.625¬µs"
I0403 07:17:29.282411       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-7779f9b69b" duration="201.25¬µs"
I0403 07:17:29.532337       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/metrics-server-7fbb699795" duration="85.959¬µs"
I0403 07:17:30.550548       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/metrics-server-7fbb699795" duration="58.25¬µs"
I0403 07:17:37.662588       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
E0403 07:17:53.830240       1 resource_quota_controller.go:446] "Unhandled Error" err="unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: stale GroupVersion discovery: metrics.k8s.io/v1beta1" logger="UnhandledError"
I0403 07:17:53.900106       1 garbagecollector.go:787] "failed to discover some groups" logger="garbage-collector-controller" groups="map[\"metrics.k8s.io/v1beta1\":\"stale GroupVersion discovery: metrics.k8s.io/v1beta1\"]"
E0403 07:18:23.836065       1 resource_quota_controller.go:446] "Unhandled Error" err="unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: stale GroupVersion discovery: metrics.k8s.io/v1beta1" logger="UnhandledError"
I0403 07:18:23.905069       1 garbagecollector.go:787] "failed to discover some groups" logger="garbage-collector-controller" groups="map[\"metrics.k8s.io/v1beta1\":\"stale GroupVersion discovery: metrics.k8s.io/v1beta1\"]"
E0403 07:18:53.843406       1 resource_quota_controller.go:446] "Unhandled Error" err="unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: stale GroupVersion discovery: metrics.k8s.io/v1beta1" logger="UnhandledError"
I0403 07:18:53.911714       1 garbagecollector.go:787] "failed to discover some groups" logger="garbage-collector-controller" groups="map[\"metrics.k8s.io/v1beta1\":\"stale GroupVersion discovery: metrics.k8s.io/v1beta1\"]"
I0403 07:19:22.684504       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/logstash-deployment-7f495c69df" duration="11.79775ms"
I0403 07:19:22.685050       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/logstash-deployment-7f495c69df" duration="32.542¬µs"
E0403 07:19:23.848690       1 resource_quota_controller.go:446] "Unhandled Error" err="unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: stale GroupVersion discovery: metrics.k8s.io/v1beta1" logger="UnhandledError"
I0403 07:19:23.923125       1 garbagecollector.go:787] "failed to discover some groups" logger="garbage-collector-controller" groups="map[\"metrics.k8s.io/v1beta1\":\"stale GroupVersion discovery: metrics.k8s.io/v1beta1\"]"
I0403 07:19:49.951089       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/logstash-deployment-7f495c69df" duration="12.018583ms"
I0403 07:19:49.951771       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/logstash-deployment-7f495c69df" duration="30.916¬µs"
I0403 07:19:52.019638       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mongo-express-deployment-5dd87b9fcf" duration="8.239041ms"
I0403 07:19:52.019766       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mongo-express-deployment-5dd87b9fcf" duration="56.25¬µs"
E0403 07:19:53.866037       1 resource_quota_controller.go:446] "Unhandled Error" err="unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: stale GroupVersion discovery: metrics.k8s.io/v1beta1" logger="UnhandledError"
I0403 07:19:53.928570       1 garbagecollector.go:787] "failed to discover some groups" logger="garbage-collector-controller" groups="map[\"metrics.k8s.io/v1beta1\":\"stale GroupVersion discovery: metrics.k8s.io/v1beta1\"]"
I0403 07:20:05.289992       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/logstash-deployment-7f495c69df" duration="415.083¬µs"
E0403 07:20:23.875240       1 resource_quota_controller.go:446] "Unhandled Error" err="unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: stale GroupVersion discovery: metrics.k8s.io/v1beta1" logger="UnhandledError"
I0403 07:20:23.936165       1 garbagecollector.go:787] "failed to discover some groups" logger="garbage-collector-controller" groups="map[\"metrics.k8s.io/v1beta1\":\"stale GroupVersion discovery: metrics.k8s.io/v1beta1\"]"
I0403 07:20:32.093013       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mongo-express-deployment-5dd87b9fcf" duration="7.877583ms"
I0403 07:20:32.093428       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mongo-express-deployment-5dd87b9fcf" duration="26.375¬µs"
I0403 07:20:44.271062       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mongo-express-deployment-5dd87b9fcf" duration="77.958¬µs"
E0403 07:20:53.883456       1 resource_quota_controller.go:446] "Unhandled Error" err="unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: stale GroupVersion discovery: metrics.k8s.io/v1beta1" logger="UnhandledError"
I0403 07:20:53.947863       1 garbagecollector.go:787] "failed to discover some groups" logger="garbage-collector-controller" groups="map[\"metrics.k8s.io/v1beta1\":\"stale GroupVersion discovery: metrics.k8s.io/v1beta1\"]"
E0403 07:21:23.890182       1 resource_quota_controller.go:446] "Unhandled Error" err="unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: stale GroupVersion discovery: metrics.k8s.io/v1beta1" logger="UnhandledError"
I0403 07:21:23.954334       1 garbagecollector.go:787] "failed to discover some groups" logger="garbage-collector-controller" groups="map[\"metrics.k8s.io/v1beta1\":\"stale GroupVersion discovery: metrics.k8s.io/v1beta1\"]"
E0403 07:21:53.898663       1 resource_quota_controller.go:446] "Unhandled Error" err="unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: stale GroupVersion discovery: metrics.k8s.io/v1beta1" logger="UnhandledError"
I0403 07:21:53.959849       1 garbagecollector.go:787] "failed to discover some groups" logger="garbage-collector-controller" groups="map[\"metrics.k8s.io/v1beta1\":\"stale GroupVersion discovery: metrics.k8s.io/v1beta1\"]"
I0403 07:22:23.362818       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-7779f9b69b" duration="148.542¬µs"


==> kube-proxy [f9d5d6f619db] <==
I0402 11:30:13.498902       1 server_linux.go:66] "Using iptables proxy"
I0402 11:30:13.561724       1 server.go:698] "Successfully retrieved node IP(s)" IPs=["192.168.49.2"]
E0402 11:30:13.561806       1 server.go:234] "Kube-proxy configuration may be incomplete or incorrect" err="nodePortAddresses is unset; NodePort connections will be accepted on all local IPs. Consider using `--nodeport-addresses primary`"
I0402 11:30:13.574750       1 server.go:243] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0402 11:30:13.574783       1 server_linux.go:170] "Using iptables Proxier"
I0402 11:30:13.575843       1 proxier.go:255] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses" ipFamily="IPv4"
I0402 11:30:13.576275       1 server.go:497] "Version info" version="v1.32.0"
I0402 11:30:13.576289       1 server.go:499] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0402 11:30:13.577141       1 config.go:199] "Starting service config controller"
I0402 11:30:13.578013       1 shared_informer.go:313] Waiting for caches to sync for service config
I0402 11:30:13.577442       1 config.go:105] "Starting endpoint slice config controller"
I0402 11:30:13.578038       1 shared_informer.go:313] Waiting for caches to sync for endpoint slice config
I0402 11:30:13.577739       1 config.go:329] "Starting node config controller"
I0402 11:30:13.578042       1 shared_informer.go:313] Waiting for caches to sync for node config
I0402 11:30:13.678940       1 shared_informer.go:320] Caches are synced for node config
I0402 11:30:13.678963       1 shared_informer.go:320] Caches are synced for service config
I0402 11:30:13.678969       1 shared_informer.go:320] Caches are synced for endpoint slice config


==> kube-scheduler [0c94e848f573] <==
I0402 11:30:04.950465       1 serving.go:386] Generated self-signed cert in-memory
W0402 11:30:05.753871       1 requestheader_controller.go:204] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W0402 11:30:05.754022       1 authentication.go:397] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W0402 11:30:05.754044       1 authentication.go:398] Continuing without authentication configuration. This may treat all requests as anonymous.
W0402 11:30:05.754056       1 authentication.go:399] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I0402 11:30:05.764869       1 server.go:166] "Starting Kubernetes Scheduler" version="v1.32.0"
I0402 11:30:05.764897       1 server.go:168] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0402 11:30:05.766590       1 configmap_cafile_content.go:205] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0402 11:30:05.766713       1 shared_informer.go:313] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0402 11:30:05.766718       1 secure_serving.go:213] Serving securely on 127.0.0.1:10259
I0402 11:30:05.766750       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
W0402 11:30:05.767492       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
E0402 11:30:05.767546       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Service: failed to list *v1.Service: services is forbidden: User \"system:kube-scheduler\" cannot list resource \"services\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0402 11:30:05.767615       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
E0402 11:30:05.767776       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"statefulsets\" in API group \"apps\" at the cluster scope" logger="UnhandledError"
W0402 11:30:05.768221       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
E0402 11:30:05.768238       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Namespace: failed to list *v1.Namespace: namespaces is forbidden: User \"system:kube-scheduler\" cannot list resource \"namespaces\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0402 11:30:05.768256       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
E0402 11:30:05.768330       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Node: failed to list *v1.Node: nodes is forbidden: User \"system:kube-scheduler\" cannot list resource \"nodes\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0402 11:30:05.768360       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
E0402 11:30:05.768367       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csistoragecapacities\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0402 11:30:05.768399       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
E0402 11:30:05.768627       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"storageclasses\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0402 11:30:05.768507       1 reflector.go:569] runtime/asm_arm64.s:1223: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
E0402 11:30:05.768716       1 reflector.go:166] "Unhandled Error" err="runtime/asm_arm64.s:1223: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps \"extension-apiserver-authentication\" is forbidden: User \"system:kube-scheduler\" cannot list resource \"configmaps\" in API group \"\" in the namespace \"kube-system\"" logger="UnhandledError"
W0402 11:30:05.768520       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
E0402 11:30:05.768725       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumes\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0402 11:30:05.768532       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
E0402 11:30:05.768761       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSINode: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csinodes\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0402 11:30:05.768566       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
W0402 11:30:05.768777       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
E0402 11:30:05.768798       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csidrivers\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0402 11:30:05.768578       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.VolumeAttachment: volumeattachments.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "volumeattachments" in API group "storage.k8s.io" at the cluster scope
E0402 11:30:05.768806       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.VolumeAttachment: failed to list *v1.VolumeAttachment: volumeattachments.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"volumeattachments\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0402 11:30:05.768586       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
E0402 11:30:05.768814       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User \"system:kube-scheduler\" cannot list resource \"poddisruptionbudgets\" in API group \"policy\" at the cluster scope" logger="UnhandledError"
E0402 11:30:05.768772       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Pod: failed to list *v1.Pod: pods is forbidden: User \"system:kube-scheduler\" cannot list resource \"pods\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0402 11:30:05.768607       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
E0402 11:30:05.768833       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicationcontrollers\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0402 11:30:05.768882       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
E0402 11:30:05.768895       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicasets\" in API group \"apps\" at the cluster scope" logger="UnhandledError"
W0402 11:30:05.768925       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
E0402 11:30:05.768937       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumeclaims\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0402 11:30:06.593414       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
E0402 11:30:06.593446       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"statefulsets\" in API group \"apps\" at the cluster scope" logger="UnhandledError"
W0402 11:30:06.636825       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
E0402 11:30:06.636859       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csistoragecapacities\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0402 11:30:06.716427       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
E0402 11:30:06.716519       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Service: failed to list *v1.Service: services is forbidden: User \"system:kube-scheduler\" cannot list resource \"services\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0402 11:30:06.784112       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
E0402 11:30:06.784195       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Node: failed to list *v1.Node: nodes is forbidden: User \"system:kube-scheduler\" cannot list resource \"nodes\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0402 11:30:06.791256       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
E0402 11:30:06.791280       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Pod: failed to list *v1.Pod: pods is forbidden: User \"system:kube-scheduler\" cannot list resource \"pods\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0402 11:30:06.826519       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
E0402 11:30:06.826587       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicationcontrollers\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0402 11:30:06.846613       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
E0402 11:30:06.846668       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User \"system:kube-scheduler\" cannot list resource \"poddisruptionbudgets\" in API group \"policy\" at the cluster scope" logger="UnhandledError"
I0402 11:30:07.367524       1 shared_informer.go:320] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file


==> kubelet <==
Apr 03 07:21:31 minikube kubelet[2313]: E0403 07:21:31.258137    2313 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"mongodb\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=mongodb pod=mongodb-stateful-0_default(67b3f3e1-26e9-459c-96f1-7f4d8c1ce02f)\"" pod="default/mongodb-stateful-0" podUID="67b3f3e1-26e9-459c-96f1-7f4d8c1ce02f"
Apr 03 07:21:31 minikube kubelet[2313]: I0403 07:21:31.258830    2313 scope.go:117] "RemoveContainer" containerID="94721584dbdefa15ecedc4250ddb31b7697cf0778b77b4137ae5ed072a9ee801"
Apr 03 07:21:31 minikube kubelet[2313]: E0403 07:21:31.260475    2313 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"metrics-server\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=metrics-server pod=metrics-server-7fbb699795-lph7c_kube-system(a261e921-f8a3-4055-a7cb-ec0147f455cd)\"" pod="kube-system/metrics-server-7fbb699795-lph7c" podUID="a261e921-f8a3-4055-a7cb-ec0147f455cd"
Apr 03 07:21:34 minikube kubelet[2313]: I0403 07:21:34.258898    2313 scope.go:117] "RemoveContainer" containerID="4fc09bd33f6b068905cfc8f5f13f1f66ab7163dfc4a820defb490e36d460fa18"
Apr 03 07:21:34 minikube kubelet[2313]: E0403 07:21:34.259081    2313 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"mongo-express\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=mongo-express pod=mongo-express-deployment-5dd87b9fcf-5pjm4_default(2a07faf0-2bb9-4de2-8a0c-136a4e615bc5)\"" pod="default/mongo-express-deployment-5dd87b9fcf-5pjm4" podUID="2a07faf0-2bb9-4de2-8a0c-136a4e615bc5"
Apr 03 07:21:35 minikube kubelet[2313]: I0403 07:21:35.257064    2313 scope.go:117] "RemoveContainer" containerID="6438ce32b1642faae1707b26688105519121c402e1fe1d83cb742803d2752016"
Apr 03 07:21:35 minikube kubelet[2313]: E0403 07:21:35.257300    2313 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"logstash\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=logstash pod=logstash-deployment-7f495c69df-pq79f_default(4b993a95-1662-4953-a5f4-d9c4d333d917)\"" pod="default/logstash-deployment-7f495c69df-pq79f" podUID="4b993a95-1662-4953-a5f4-d9c4d333d917"
Apr 03 07:21:36 minikube kubelet[2313]: I0403 07:21:36.256766    2313 scope.go:117] "RemoveContainer" containerID="1d9bef314f7cae0149ccac75fbf6d15b94039e631e8733a7b3706f1770053ee2"
Apr 03 07:21:36 minikube kubelet[2313]: E0403 07:21:36.256942    2313 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"dashboard-metrics-scraper\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=dashboard-metrics-scraper pod=dashboard-metrics-scraper-5d59dccf9b-gnc7p_kubernetes-dashboard(b169962f-2958-4dee-9e43-d439976e402a)\"" pod="kubernetes-dashboard/dashboard-metrics-scraper-5d59dccf9b-gnc7p" podUID="b169962f-2958-4dee-9e43-d439976e402a"
Apr 03 07:21:36 minikube kubelet[2313]: E0403 07:21:36.258719    2313 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"kubernetes-dashboard\" with ImagePullBackOff: \"Back-off pulling image \\\"docker.io/kubernetesui/dashboard:v2.7.0@sha256:2e500d29e9d5f4a086b908eb8dfe7ecac57d2ab09d65b24f588b1d449841ef93\\\": ErrImagePull: failed to register layer: write /public/fr/Roboto-Black.2eaa390d458c877d.woff2: no space left on device\"" pod="kubernetes-dashboard/kubernetes-dashboard-7779f9b69b-b5wqm" podUID="e8a54c36-f198-4a6e-a8ad-07c1bbd339e4"
Apr 03 07:21:40 minikube kubelet[2313]: E0403 07:21:40.917165    2313 log.go:32] "ReopenContainerLog from runtime service failed" err="rpc error: code = Unknown desc = docker does not support reopening container log files" containerID="f5c6a3a9c166a3c551fbbd196409ab6662b8977963893c9fe0c4f9d3a0468064"
Apr 03 07:21:40 minikube kubelet[2313]: E0403 07:21:40.917267    2313 container_log_manager.go:307] "Failed to rotate log for container" err="failed to rotate log \"/var/log/pods/kube-system_coredns-668d6bf9bc-ln8dv_952fdd87-6ea7-4791-ab0f-5e6d7708c7ee/coredns/0.log\": failed to reopen container log \"f5c6a3a9c166a3c551fbbd196409ab6662b8977963893c9fe0c4f9d3a0468064\": rpc error: code = Unknown desc = docker does not support reopening container log files" worker=1 containerID="f5c6a3a9c166a3c551fbbd196409ab6662b8977963893c9fe0c4f9d3a0468064" path="/var/log/pods/kube-system_coredns-668d6bf9bc-ln8dv_952fdd87-6ea7-4791-ab0f-5e6d7708c7ee/coredns/0.log" currentSize=25416190 maxSize=10485760
Apr 03 07:21:42 minikube kubelet[2313]: I0403 07:21:42.259381    2313 scope.go:117] "RemoveContainer" containerID="94721584dbdefa15ecedc4250ddb31b7697cf0778b77b4137ae5ed072a9ee801"
Apr 03 07:21:42 minikube kubelet[2313]: I0403 07:21:42.259696    2313 scope.go:117] "RemoveContainer" containerID="2181e223bf3b52d02babf7f4796c42119c384f15ba1dce34a1c16571f8751f83"
Apr 03 07:21:42 minikube kubelet[2313]: E0403 07:21:42.259849    2313 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"metrics-server\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=metrics-server pod=metrics-server-7fbb699795-lph7c_kube-system(a261e921-f8a3-4055-a7cb-ec0147f455cd)\"" pod="kube-system/metrics-server-7fbb699795-lph7c" podUID="a261e921-f8a3-4055-a7cb-ec0147f455cd"
Apr 03 07:21:42 minikube kubelet[2313]: E0403 07:21:42.259953    2313 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"mongodb\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=mongodb pod=mongodb-stateful-0_default(67b3f3e1-26e9-459c-96f1-7f4d8c1ce02f)\"" pod="default/mongodb-stateful-0" podUID="67b3f3e1-26e9-459c-96f1-7f4d8c1ce02f"
Apr 03 07:21:47 minikube kubelet[2313]: I0403 07:21:47.256665    2313 scope.go:117] "RemoveContainer" containerID="1d9bef314f7cae0149ccac75fbf6d15b94039e631e8733a7b3706f1770053ee2"
Apr 03 07:21:47 minikube kubelet[2313]: E0403 07:21:47.256973    2313 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"dashboard-metrics-scraper\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=dashboard-metrics-scraper pod=dashboard-metrics-scraper-5d59dccf9b-gnc7p_kubernetes-dashboard(b169962f-2958-4dee-9e43-d439976e402a)\"" pod="kubernetes-dashboard/dashboard-metrics-scraper-5d59dccf9b-gnc7p" podUID="b169962f-2958-4dee-9e43-d439976e402a"
Apr 03 07:21:47 minikube kubelet[2313]: E0403 07:21:47.259497    2313 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"kubernetes-dashboard\" with ImagePullBackOff: \"Back-off pulling image \\\"docker.io/kubernetesui/dashboard:v2.7.0@sha256:2e500d29e9d5f4a086b908eb8dfe7ecac57d2ab09d65b24f588b1d449841ef93\\\": ErrImagePull: failed to register layer: write /public/fr/Roboto-Black.2eaa390d458c877d.woff2: no space left on device\"" pod="kubernetes-dashboard/kubernetes-dashboard-7779f9b69b-b5wqm" podUID="e8a54c36-f198-4a6e-a8ad-07c1bbd339e4"
Apr 03 07:21:48 minikube kubelet[2313]: I0403 07:21:48.256371    2313 scope.go:117] "RemoveContainer" containerID="6438ce32b1642faae1707b26688105519121c402e1fe1d83cb742803d2752016"
Apr 03 07:21:48 minikube kubelet[2313]: E0403 07:21:48.256583    2313 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"logstash\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=logstash pod=logstash-deployment-7f495c69df-pq79f_default(4b993a95-1662-4953-a5f4-d9c4d333d917)\"" pod="default/logstash-deployment-7f495c69df-pq79f" podUID="4b993a95-1662-4953-a5f4-d9c4d333d917"
Apr 03 07:21:49 minikube kubelet[2313]: I0403 07:21:49.256688    2313 scope.go:117] "RemoveContainer" containerID="4fc09bd33f6b068905cfc8f5f13f1f66ab7163dfc4a820defb490e36d460fa18"
Apr 03 07:21:49 minikube kubelet[2313]: E0403 07:21:49.256833    2313 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"mongo-express\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=mongo-express pod=mongo-express-deployment-5dd87b9fcf-5pjm4_default(2a07faf0-2bb9-4de2-8a0c-136a4e615bc5)\"" pod="default/mongo-express-deployment-5dd87b9fcf-5pjm4" podUID="2a07faf0-2bb9-4de2-8a0c-136a4e615bc5"
Apr 03 07:21:50 minikube kubelet[2313]: E0403 07:21:50.930811    2313 log.go:32] "ReopenContainerLog from runtime service failed" err="rpc error: code = Unknown desc = docker does not support reopening container log files" containerID="f5c6a3a9c166a3c551fbbd196409ab6662b8977963893c9fe0c4f9d3a0468064"
Apr 03 07:21:50 minikube kubelet[2313]: E0403 07:21:50.930949    2313 container_log_manager.go:307] "Failed to rotate log for container" err="failed to rotate log \"/var/log/pods/kube-system_coredns-668d6bf9bc-ln8dv_952fdd87-6ea7-4791-ab0f-5e6d7708c7ee/coredns/0.log\": failed to reopen container log \"f5c6a3a9c166a3c551fbbd196409ab6662b8977963893c9fe0c4f9d3a0468064\": rpc error: code = Unknown desc = docker does not support reopening container log files" worker=1 containerID="f5c6a3a9c166a3c551fbbd196409ab6662b8977963893c9fe0c4f9d3a0468064" path="/var/log/pods/kube-system_coredns-668d6bf9bc-ln8dv_952fdd87-6ea7-4791-ab0f-5e6d7708c7ee/coredns/0.log" currentSize=25419316 maxSize=10485760
Apr 03 07:21:55 minikube kubelet[2313]: I0403 07:21:55.256616    2313 scope.go:117] "RemoveContainer" containerID="94721584dbdefa15ecedc4250ddb31b7697cf0778b77b4137ae5ed072a9ee801"
Apr 03 07:21:55 minikube kubelet[2313]: E0403 07:21:55.257020    2313 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"metrics-server\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=metrics-server pod=metrics-server-7fbb699795-lph7c_kube-system(a261e921-f8a3-4055-a7cb-ec0147f455cd)\"" pod="kube-system/metrics-server-7fbb699795-lph7c" podUID="a261e921-f8a3-4055-a7cb-ec0147f455cd"
Apr 03 07:21:57 minikube kubelet[2313]: I0403 07:21:57.256312    2313 scope.go:117] "RemoveContainer" containerID="2181e223bf3b52d02babf7f4796c42119c384f15ba1dce34a1c16571f8751f83"
Apr 03 07:21:57 minikube kubelet[2313]: E0403 07:21:57.256548    2313 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"mongodb\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=mongodb pod=mongodb-stateful-0_default(67b3f3e1-26e9-459c-96f1-7f4d8c1ce02f)\"" pod="default/mongodb-stateful-0" podUID="67b3f3e1-26e9-459c-96f1-7f4d8c1ce02f"
Apr 03 07:21:58 minikube kubelet[2313]: I0403 07:21:58.256870    2313 scope.go:117] "RemoveContainer" containerID="1d9bef314f7cae0149ccac75fbf6d15b94039e631e8733a7b3706f1770053ee2"
Apr 03 07:21:58 minikube kubelet[2313]: E0403 07:21:58.257067    2313 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"dashboard-metrics-scraper\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=dashboard-metrics-scraper pod=dashboard-metrics-scraper-5d59dccf9b-gnc7p_kubernetes-dashboard(b169962f-2958-4dee-9e43-d439976e402a)\"" pod="kubernetes-dashboard/dashboard-metrics-scraper-5d59dccf9b-gnc7p" podUID="b169962f-2958-4dee-9e43-d439976e402a"
Apr 03 07:21:59 minikube kubelet[2313]: I0403 07:21:59.257583    2313 scope.go:117] "RemoveContainer" containerID="6438ce32b1642faae1707b26688105519121c402e1fe1d83cb742803d2752016"
Apr 03 07:21:59 minikube kubelet[2313]: E0403 07:21:59.257877    2313 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"logstash\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=logstash pod=logstash-deployment-7f495c69df-pq79f_default(4b993a95-1662-4953-a5f4-d9c4d333d917)\"" pod="default/logstash-deployment-7f495c69df-pq79f" podUID="4b993a95-1662-4953-a5f4-d9c4d333d917"
Apr 03 07:22:00 minikube kubelet[2313]: E0403 07:22:00.929145    2313 log.go:32] "ReopenContainerLog from runtime service failed" err="rpc error: code = Unknown desc = docker does not support reopening container log files" containerID="f5c6a3a9c166a3c551fbbd196409ab6662b8977963893c9fe0c4f9d3a0468064"
Apr 03 07:22:00 minikube kubelet[2313]: E0403 07:22:00.929754    2313 container_log_manager.go:307] "Failed to rotate log for container" err="failed to rotate log \"/var/log/pods/kube-system_coredns-668d6bf9bc-ln8dv_952fdd87-6ea7-4791-ab0f-5e6d7708c7ee/coredns/0.log\": failed to reopen container log \"f5c6a3a9c166a3c551fbbd196409ab6662b8977963893c9fe0c4f9d3a0468064\": rpc error: code = Unknown desc = docker does not support reopening container log files" worker=1 containerID="f5c6a3a9c166a3c551fbbd196409ab6662b8977963893c9fe0c4f9d3a0468064" path="/var/log/pods/kube-system_coredns-668d6bf9bc-ln8dv_952fdd87-6ea7-4791-ab0f-5e6d7708c7ee/coredns/0.log" currentSize=25423290 maxSize=10485760
Apr 03 07:22:04 minikube kubelet[2313]: I0403 07:22:04.246730    2313 scope.go:117] "RemoveContainer" containerID="4fc09bd33f6b068905cfc8f5f13f1f66ab7163dfc4a820defb490e36d460fa18"
Apr 03 07:22:04 minikube kubelet[2313]: E0403 07:22:04.248633    2313 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"mongo-express\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=mongo-express pod=mongo-express-deployment-5dd87b9fcf-5pjm4_default(2a07faf0-2bb9-4de2-8a0c-136a4e615bc5)\"" pod="default/mongo-express-deployment-5dd87b9fcf-5pjm4" podUID="2a07faf0-2bb9-4de2-8a0c-136a4e615bc5"
Apr 03 07:22:07 minikube kubelet[2313]: I0403 07:22:07.257005    2313 scope.go:117] "RemoveContainer" containerID="94721584dbdefa15ecedc4250ddb31b7697cf0778b77b4137ae5ed072a9ee801"
Apr 03 07:22:07 minikube kubelet[2313]: E0403 07:22:07.257581    2313 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"metrics-server\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=metrics-server pod=metrics-server-7fbb699795-lph7c_kube-system(a261e921-f8a3-4055-a7cb-ec0147f455cd)\"" pod="kube-system/metrics-server-7fbb699795-lph7c" podUID="a261e921-f8a3-4055-a7cb-ec0147f455cd"
Apr 03 07:22:09 minikube kubelet[2313]: E0403 07:22:09.591522    2313 log.go:32] "PullImage from image service failed" err="rpc error: code = Unknown desc = failed to register layer: write /public/fr/MaterialIcons-Regular.1e50f5c2ffa6aba4.eot: no space left on device" image="docker.io/kubernetesui/dashboard:v2.7.0@sha256:2e500d29e9d5f4a086b908eb8dfe7ecac57d2ab09d65b24f588b1d449841ef93"
Apr 03 07:22:09 minikube kubelet[2313]: E0403 07:22:09.591730    2313 kuberuntime_image.go:55] "Failed to pull image" err="failed to register layer: write /public/fr/MaterialIcons-Regular.1e50f5c2ffa6aba4.eot: no space left on device" image="docker.io/kubernetesui/dashboard:v2.7.0@sha256:2e500d29e9d5f4a086b908eb8dfe7ecac57d2ab09d65b24f588b1d449841ef93"
Apr 03 07:22:09 minikube kubelet[2313]: E0403 07:22:09.592499    2313 kuberuntime_manager.go:1341] "Unhandled Error" err="container &Container{Name:kubernetes-dashboard,Image:docker.io/kubernetesui/dashboard:v2.7.0@sha256:2e500d29e9d5f4a086b908eb8dfe7ecac57d2ab09d65b24f588b1d449841ef93,Command:[],Args:[--namespace=kubernetes-dashboard --enable-skip-login --disable-settings-authorizer],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:9090,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:tmp-volume,ReadOnly:false,MountPath:/tmp,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},VolumeMount{Name:kube-api-access-7mqc8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:&Probe{ProbeHandler:ProbeHandler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/,Port:{0 9090 },Host:,Scheme:HTTP,HTTPHeaders:[]HTTPHeader{},},TCPSocket:nil,GRPC:nil,},InitialDelaySeconds:30,TimeoutSeconds:30,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,TerminationGracePeriodSeconds:nil,},ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:*1001,RunAsNonRoot:nil,ReadOnlyRootFilesystem:*true,AllowPrivilegeEscalation:*false,RunAsGroup:*2001,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,AppArmorProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod kubernetes-dashboard-7779f9b69b-b5wqm_kubernetes-dashboard(e8a54c36-f198-4a6e-a8ad-07c1bbd339e4): ErrImagePull: failed to register layer: write /public/fr/MaterialIcons-Regular.1e50f5c2ffa6aba4.eot: no space left on device" logger="UnhandledError"
Apr 03 07:22:09 minikube kubelet[2313]: E0403 07:22:09.593852    2313 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"kubernetes-dashboard\" with ErrImagePull: \"failed to register layer: write /public/fr/MaterialIcons-Regular.1e50f5c2ffa6aba4.eot: no space left on device\"" pod="kubernetes-dashboard/kubernetes-dashboard-7779f9b69b-b5wqm" podUID="e8a54c36-f198-4a6e-a8ad-07c1bbd339e4"
Apr 03 07:22:10 minikube kubelet[2313]: E0403 07:22:10.940480    2313 log.go:32] "ReopenContainerLog from runtime service failed" err="rpc error: code = Unknown desc = docker does not support reopening container log files" containerID="f5c6a3a9c166a3c551fbbd196409ab6662b8977963893c9fe0c4f9d3a0468064"
Apr 03 07:22:10 minikube kubelet[2313]: E0403 07:22:10.940762    2313 container_log_manager.go:307] "Failed to rotate log for container" err="failed to rotate log \"/var/log/pods/kube-system_coredns-668d6bf9bc-ln8dv_952fdd87-6ea7-4791-ab0f-5e6d7708c7ee/coredns/0.log\": failed to reopen container log \"f5c6a3a9c166a3c551fbbd196409ab6662b8977963893c9fe0c4f9d3a0468064\": rpc error: code = Unknown desc = docker does not support reopening container log files" worker=1 containerID="f5c6a3a9c166a3c551fbbd196409ab6662b8977963893c9fe0c4f9d3a0468064" path="/var/log/pods/kube-system_coredns-668d6bf9bc-ln8dv_952fdd87-6ea7-4791-ab0f-5e6d7708c7ee/coredns/0.log" currentSize=25426409 maxSize=10485760
Apr 03 07:22:11 minikube kubelet[2313]: I0403 07:22:11.293508    2313 scope.go:117] "RemoveContainer" containerID="2181e223bf3b52d02babf7f4796c42119c384f15ba1dce34a1c16571f8751f83"
Apr 03 07:22:11 minikube kubelet[2313]: E0403 07:22:11.345120    2313 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"mongodb\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=mongodb pod=mongodb-stateful-0_default(67b3f3e1-26e9-459c-96f1-7f4d8c1ce02f)\"" pod="default/mongodb-stateful-0" podUID="67b3f3e1-26e9-459c-96f1-7f4d8c1ce02f"
Apr 03 07:22:12 minikube kubelet[2313]: I0403 07:22:12.314704    2313 scope.go:117] "RemoveContainer" containerID="6438ce32b1642faae1707b26688105519121c402e1fe1d83cb742803d2752016"
Apr 03 07:22:12 minikube kubelet[2313]: E0403 07:22:12.314971    2313 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"logstash\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=logstash pod=logstash-deployment-7f495c69df-pq79f_default(4b993a95-1662-4953-a5f4-d9c4d333d917)\"" pod="default/logstash-deployment-7f495c69df-pq79f" podUID="4b993a95-1662-4953-a5f4-d9c4d333d917"
Apr 03 07:22:13 minikube kubelet[2313]: I0403 07:22:13.254570    2313 scope.go:117] "RemoveContainer" containerID="1d9bef314f7cae0149ccac75fbf6d15b94039e631e8733a7b3706f1770053ee2"
Apr 03 07:22:13 minikube kubelet[2313]: E0403 07:22:13.254706    2313 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"dashboard-metrics-scraper\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=dashboard-metrics-scraper pod=dashboard-metrics-scraper-5d59dccf9b-gnc7p_kubernetes-dashboard(b169962f-2958-4dee-9e43-d439976e402a)\"" pod="kubernetes-dashboard/dashboard-metrics-scraper-5d59dccf9b-gnc7p" podUID="b169962f-2958-4dee-9e43-d439976e402a"
Apr 03 07:22:17 minikube kubelet[2313]: I0403 07:22:17.254680    2313 scope.go:117] "RemoveContainer" containerID="4fc09bd33f6b068905cfc8f5f13f1f66ab7163dfc4a820defb490e36d460fa18"
Apr 03 07:22:17 minikube kubelet[2313]: E0403 07:22:17.255176    2313 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"mongo-express\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=mongo-express pod=mongo-express-deployment-5dd87b9fcf-5pjm4_default(2a07faf0-2bb9-4de2-8a0c-136a4e615bc5)\"" pod="default/mongo-express-deployment-5dd87b9fcf-5pjm4" podUID="2a07faf0-2bb9-4de2-8a0c-136a4e615bc5"
Apr 03 07:22:20 minikube kubelet[2313]: I0403 07:22:20.255317    2313 scope.go:117] "RemoveContainer" containerID="94721584dbdefa15ecedc4250ddb31b7697cf0778b77b4137ae5ed072a9ee801"
Apr 03 07:22:20 minikube kubelet[2313]: E0403 07:22:20.255711    2313 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"metrics-server\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=metrics-server pod=metrics-server-7fbb699795-lph7c_kube-system(a261e921-f8a3-4055-a7cb-ec0147f455cd)\"" pod="kube-system/metrics-server-7fbb699795-lph7c" podUID="a261e921-f8a3-4055-a7cb-ec0147f455cd"
Apr 03 07:22:20 minikube kubelet[2313]: E0403 07:22:20.995459    2313 log.go:32] "ReopenContainerLog from runtime service failed" err="rpc error: code = Unknown desc = docker does not support reopening container log files" containerID="f5c6a3a9c166a3c551fbbd196409ab6662b8977963893c9fe0c4f9d3a0468064"
Apr 03 07:22:20 minikube kubelet[2313]: E0403 07:22:20.996031    2313 container_log_manager.go:307] "Failed to rotate log for container" err="failed to rotate log \"/var/log/pods/kube-system_coredns-668d6bf9bc-ln8dv_952fdd87-6ea7-4791-ab0f-5e6d7708c7ee/coredns/0.log\": failed to reopen container log \"f5c6a3a9c166a3c551fbbd196409ab6662b8977963893c9fe0c4f9d3a0468064\": rpc error: code = Unknown desc = docker does not support reopening container log files" worker=1 containerID="f5c6a3a9c166a3c551fbbd196409ab6662b8977963893c9fe0c4f9d3a0468064" path="/var/log/pods/kube-system_coredns-668d6bf9bc-ln8dv_952fdd87-6ea7-4791-ab0f-5e6d7708c7ee/coredns/0.log" currentSize=25429525 maxSize=10485760
Apr 03 07:22:22 minikube kubelet[2313]: I0403 07:22:22.256162    2313 scope.go:117] "RemoveContainer" containerID="2181e223bf3b52d02babf7f4796c42119c384f15ba1dce34a1c16571f8751f83"
Apr 03 07:22:22 minikube kubelet[2313]: E0403 07:22:22.256404    2313 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"mongodb\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=mongodb pod=mongodb-stateful-0_default(67b3f3e1-26e9-459c-96f1-7f4d8c1ce02f)\"" pod="default/mongodb-stateful-0" podUID="67b3f3e1-26e9-459c-96f1-7f4d8c1ce02f"
Apr 03 07:22:23 minikube kubelet[2313]: E0403 07:22:23.346639    2313 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"kubernetes-dashboard\" with ImagePullBackOff: \"Back-off pulling image \\\"docker.io/kubernetesui/dashboard:v2.7.0@sha256:2e500d29e9d5f4a086b908eb8dfe7ecac57d2ab09d65b24f588b1d449841ef93\\\": ErrImagePull: failed to register layer: write /public/fr/MaterialIcons-Regular.1e50f5c2ffa6aba4.eot: no space left on device\"" pod="kubernetes-dashboard/kubernetes-dashboard-7779f9b69b-b5wqm" podUID="e8a54c36-f198-4a6e-a8ad-07c1bbd339e4"


==> storage-provisioner [52b96f69b3d7] <==
I0402 15:47:31.316852       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
I0402 15:47:31.495579       1 storage_provisioner.go:141] Storage provisioner initialized, now starting service!
I0402 15:47:31.495850       1 leaderelection.go:243] attempting to acquire leader lease kube-system/k8s.io-minikube-hostpath...
I0402 15:47:49.515251       1 leaderelection.go:253] successfully acquired lease kube-system/k8s.io-minikube-hostpath
I0402 15:47:49.515962       1 event.go:282] Event(v1.ObjectReference{Kind:"Endpoints", Namespace:"kube-system", Name:"k8s.io-minikube-hostpath", UID:"e40c28cf-4ced-46a6-a0e0-d94f24e031f9", APIVersion:"v1", ResourceVersion:"11182", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' minikube_7831af19-6e3f-42e2-b987-83b3319b82e5 became leader
I0402 15:47:49.516345       1 controller.go:835] Starting provisioner controller k8s.io/minikube-hostpath_minikube_7831af19-6e3f-42e2-b987-83b3319b82e5!
I0402 15:47:49.617950       1 controller.go:884] Started provisioner controller k8s.io/minikube-hostpath_minikube_7831af19-6e3f-42e2-b987-83b3319b82e5!


==> storage-provisioner [bfebc580ea36] <==
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/util/wait/wait.go:155 +0x64
k8s.io/apimachinery/pkg/util/wait.BackoffUntil(0x40000b60e0, 0x1267368, 0x4000358150, 0x1, 0x400007ec60)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/util/wait/wait.go:156 +0x74
k8s.io/apimachinery/pkg/util/wait.JitterUntil(0x40000b60e0, 0x3b9aca00, 0x0, 0x1, 0x400007ec60)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/util/wait/wait.go:133 +0x88
k8s.io/apimachinery/pkg/util/wait.Until(0x40000b60e0, 0x3b9aca00, 0x400007ec60)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/util/wait/wait.go:90 +0x48
created by sigs.k8s.io/sig-storage-lib-external-provisioner/v6/controller.(*ProvisionController).Run.func1
	/Users/medya/go/pkg/mod/sigs.k8s.io/sig-storage-lib-external-provisioner/v6@v6.3.0/controller/controller.go:880 +0x3c8

goroutine 115 [sync.Cond.Wait, 6 minutes]:
sync.runtime_notifyListWait(0x4000496b10, 0x0)
	/usr/local/go/src/runtime/sema.go:513 +0xf8
sync.(*Cond).Wait(0x4000496b00)
	/usr/local/go/src/sync/cond.go:56 +0xb8
k8s.io/client-go/util/workqueue.(*Type).Get(0x40004c4360, 0x0, 0x0, 0x1c200)
	/Users/medya/go/pkg/mod/k8s.io/client-go@v0.20.5/util/workqueue/queue.go:145 +0x84
sigs.k8s.io/sig-storage-lib-external-provisioner/v6/controller.(*ProvisionController).processNextVolumeWorkItem(0x4000470a00, 0x1298cd0, 0x4000496dc0, 0x1)
	/Users/medya/go/pkg/mod/sigs.k8s.io/sig-storage-lib-external-provisioner/v6@v6.3.0/controller/controller.go:990 +0x34
sigs.k8s.io/sig-storage-lib-external-provisioner/v6/controller.(*ProvisionController).runVolumeWorker(...)
	/Users/medya/go/pkg/mod/sigs.k8s.io/sig-storage-lib-external-provisioner/v6@v6.3.0/controller/controller.go:929
sigs.k8s.io/sig-storage-lib-external-provisioner/v6/controller.(*ProvisionController).Run.func1.3()
	/Users/medya/go/pkg/mod/sigs.k8s.io/sig-storage-lib-external-provisioner/v6@v6.3.0/controller/controller.go:881 +0x54
k8s.io/apimachinery/pkg/util/wait.BackoffUntil.func1(0x40000b6100)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/util/wait/wait.go:155 +0x64
k8s.io/apimachinery/pkg/util/wait.BackoffUntil(0x40000b6100, 0x1267368, 0x4000554cf0, 0x1, 0x400007ec60)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/util/wait/wait.go:156 +0x74
k8s.io/apimachinery/pkg/util/wait.JitterUntil(0x40000b6100, 0x3b9aca00, 0x0, 0x1, 0x400007ec60)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/util/wait/wait.go:133 +0x88
k8s.io/apimachinery/pkg/util/wait.Until(0x40000b6100, 0x3b9aca00, 0x400007ec60)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/util/wait/wait.go:90 +0x48
created by sigs.k8s.io/sig-storage-lib-external-provisioner/v6/controller.(*ProvisionController).Run.func1
	/Users/medya/go/pkg/mod/sigs.k8s.io/sig-storage-lib-external-provisioner/v6@v6.3.0/controller/controller.go:881 +0x308

goroutine 564 [sync.Cond.Wait]:
sync.runtime_notifyListWait(0x40004cb380, 0xffff00000001)
	/usr/local/go/src/runtime/sema.go:513 +0xf8
sync.(*Cond).Wait(0x40004cb370)
	/usr/local/go/src/sync/cond.go:56 +0xb8
golang.org/x/net/http2.(*pipe).Read(0x40004cb368, 0x40002bc601, 0x5ff, 0x5ff, 0x0, 0x0, 0x0)
	/Users/medya/go/pkg/mod/golang.org/x/net@v0.0.0-20201224014010-6772e930b67b/http2/pipe.go:65 +0x94
golang.org/x/net/http2.transportResponseBody.Read(0x40004cb340, 0x40002bc601, 0x5ff, 0x5ff, 0x0, 0x0, 0x0)
	/Users/medya/go/pkg/mod/golang.org/x/net@v0.0.0-20201224014010-6772e930b67b/http2/transport.go:2108 +0x80
encoding/json.(*Decoder).refill(0x40004cb600, 0xa, 0x9)
	/usr/local/go/src/encoding/json/stream.go:165 +0xd4
encoding/json.(*Decoder).readValue(0x40004cb600, 0x0, 0x0, 0x7952ac)
	/usr/local/go/src/encoding/json/stream.go:140 +0x1b8
encoding/json.(*Decoder).Decode(0x40004cb600, 0xefe300, 0x400036b0f8, 0x0, 0x0)
	/usr/local/go/src/encoding/json/stream.go:63 +0x60
k8s.io/apimachinery/pkg/util/framer.(*jsonFrameReader).Read(0x4000216cf0, 0x40004c8c00, 0x400, 0x400, 0x400009fd00, 0x15400, 0x400009fd00)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/util/framer/framer.go:152 +0x194
k8s.io/apimachinery/pkg/runtime/serializer/streaming.(*decoder).Decode(0x40005e85f0, 0x0, 0x126f7a8, 0x40004971c0, 0x0, 0x0, 0x6cad0, 0x40004c06c0, 0x4000685648)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/runtime/serializer/streaming/streaming.go:77 +0x70
k8s.io/client-go/rest/watch.(*Decoder).Decode(0x400035f600, 0x400009fef0, 0x8, 0x126f1e0, 0x40001c7800, 0x0, 0x0)
	/Users/medya/go/pkg/mod/k8s.io/client-go@v0.20.5/rest/watch/decoder.go:49 +0x60
k8s.io/apimachinery/pkg/watch.(*StreamWatcher).receive(0x4000363540)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/watch/streamwatcher.go:104 +0xe8
created by k8s.io/apimachinery/pkg/watch.NewStreamWatcher
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/watch/streamwatcher.go:71 +0xb4
I0402 15:47:10.992686       1 event.go:282] Event(v1.ObjectReference{Kind:"Endpoints", Namespace:"kube-system", Name:"k8s.io-minikube-hostpath", UID:"e40c28cf-4ced-46a6-a0e0-d94f24e031f9", APIVersion:"v1", ResourceVersion:"11128", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' minikube_f065de34-5088-4cbb-868e-10bfa6a37c76 stopped leading

